<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks</title>
<!--Generated on Sun Oct  5 21:31:03 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.04374v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S1" title="In GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S2" title="In GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Task Creation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S2.SS1" title="In 2 Task Creation ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Prioritizing occupations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S2.SS2" title="In 2 Task Creation ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Expert Recruitment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S2.SS3" title="In 2 Task Creation ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Task Creation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S2.SS4" title="In 2 Task Creation ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Task Quality Control Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S2.SS5" title="In 2 Task Creation ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Human Expert Grading and Automated Grading</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3" title="In GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments and Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.SS1" title="In 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Headline results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.SS2" title="In 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Speed and cost comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.SS3" title="In 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Model strengths and weaknesses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.SS4" title="In 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Increasing reasoning effort and scaffolding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S4" title="In GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Open-sourcing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S5" title="In GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S5.SS0.SSS0.Px1" title="In 5 Limitations ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Dataset size:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S5.SS0.SSS0.Px2" title="In 5 Limitations ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Focus on self-contained knowledge work:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S5.SS0.SSS0.Px3" title="In 5 Limitations ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Tasks are precisely-specified and one-shot, not interactive:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S5.SS0.SSS0.Px4" title="In 5 Limitations ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Grader performance:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S5.SS0.SSS0.Px5" title="In 5 Limitations ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Cost:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S6" title="In GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1" title="In GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS1" title="In Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Disclosures</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS1.SSS1" title="In A.1 Disclosures ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>AI disclosure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS1.SSS2" title="In A.1 Disclosures ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.2 </span>Sensitive content and political content disclosure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS1.SSS3" title="In A.1 Disclosures ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.3 </span>Third-party references disclosure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2" title="In Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Additional Detail on Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS1" title="In A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Speed and Cost Analysis, continued</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS2" title="In A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.2 </span>Win rates by sector</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS3" title="In A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.3 </span>Win rates by occupation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS4" title="In A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.4 </span>Win rates by deliverable</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS5" title="In A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.5 </span>Win rates by time to complete</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS6" title="In A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.6 </span>Additional detail on model failures analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS7" title="In A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.7 </span>Under-contextualized GDPval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS3" title="In Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Additional Detail on Prompt-Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS4" title="In Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Additional Task Characteristics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS4.SSS1" title="In A.4 Additional Task Characteristics ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.1 </span>Files and attachments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS4.SSS2" title="In A.4 Additional Task Characteristics ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.2 </span>O*NET Tasks, Skills, and Work Activities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS4.SSS3" title="In A.4 Additional Task Characteristics ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.3 </span>Task specification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS4.SSS4" title="In A.4 Additional Task Characteristics ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4.4 </span>Task Representativeness</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS5" title="In Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Further Detail on Task Quality Control</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS5.SSS1" title="In A.5 Further Detail on Task Quality Control ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.1 </span>Model-in-the-loop Task Review</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS5.SSS2" title="In A.5 Further Detail on Task Quality Control ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.2 </span>Human Expert Reviewers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS5.SSS3" title="In A.5 Further Detail on Task Quality Control ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5.3 </span>Iterative review process</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6" title="In Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Automated Grader Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS1" title="In A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6.1 </span>Automated Grader Consensus Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS1.Px1" title="In A.6.1 Automated Grader Consensus Metrics ‣ A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Human-automated grader Agreement.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS1.Px2" title="In A.6.1 Automated Grader Consensus Metrics ‣ A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Human Inter-Rater Agreement.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS2" title="In A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6.2 </span>Automated Grader Correlation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS3" title="In A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6.3 </span>Automated Grader Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS4" title="In A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6.4 </span>automated grader Packages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7" title="In Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7 </span>Further Methodological Details on Selecting Occupations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7.SSS0.Px1" title="In A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Occupations in GDPval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7.SSS0.Px2" title="In A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Calculation of Total Wages Earned by Occupation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7.SSS0.Px3" title="In A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Classifying Occupations as Digital.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7.SSS0.Px4" title="In A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title">Handling Missing Data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7.SSS1" title="In A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7.1 </span>Validating the Digital Tasks Measure</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:397.5pt;">
<span class="ltx_p ltx_align_center"><span class="ltx_text ltx_font_bold">Tejal Patwardhan</span>  <span class="ltx_text ltx_font_bold">Rachel Dias<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>  <span class="ltx_text ltx_font_bold">Elizabeth Proehl<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>  <span class="ltx_text ltx_font_bold">Grace Kim<span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>  <span class="ltx_text ltx_font_bold">Michele Wang<span class="ltx_note ltx_role_footnotemark" id="footnotex4"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>  
<br class="ltx_break"/><span class="ltx_text ltx_font_bold">Olivia Watkins<span class="ltx_note ltx_role_footnotemark" id="footnotex5"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>  <span class="ltx_text ltx_font_bold">Simón Posada Fishman<span class="ltx_note ltx_role_footnotemark" id="footnotex6"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>  <span class="ltx_text ltx_font_bold">Marwan Aljubeh<span class="ltx_note ltx_role_footnotemark" id="footnotex7"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>  <span class="ltx_text ltx_font_bold">Phoebe Thacker<span class="ltx_note ltx_role_footnotemark" id="footnotex8"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span></span></span></span></span>
<br class="ltx_break"/> Laurance Fauconnet  Natalie S. Kim  Patrick Chao  Samuel Miserendino  Gildas Chabot  David Li  Michael Sharman  Alexandra Barr  Amelia Glaese  Jerry Tworek</span>
<span class="ltx_p ltx_align_center">OpenAI</span>
</span>
</span><span class="ltx_author_notes">Equal contribution. Correspondence to tejal@openai.com.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We introduce GDPval, a benchmark evaluating AI model capabilities on real-world economically valuable tasks. GDPval covers the majority of U.S. Bureau of Labor Statistics Work Activities for 44 occupations across the top 9 sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are constructed from the representative work of industry professionals with an average of 14 years of experience. We find that frontier model performance on GDPval is improving roughly linearly over time, and that the current best frontier models are approaching industry experts in deliverable quality. We analyze the potential for frontier models, when paired with human oversight, to perform GDPval tasks cheaper and faster than unaided experts. We also demonstrate that increased reasoning effort, increased task context, and increased scaffolding improves model performance on GDPval. Finally, we open-source a gold subset of 220 tasks and provide a public automated grading service at <a class="ltx_ref ltx_href" href="evals.openai.com" title="">evals.openai.com</a> to facilitate future research in understanding real-world model capabilities.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">There is growing debate about how increasingly capable AI models could affect the labor market—
whether by automating specific tasks, replacing entire occupations, or creating entirely new kinds
of work <cite class="ltx_cite ltx_citemacro_citep">(Brynjolfsson et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib7" title="">2025</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib9" title="">2025</a>)</cite>. Current approaches to measure the economic impact of AI focus on indicators such as adoption rates, usage patterns, and GDP growth attributed to AI <cite class="ltx_cite ltx_citemacro_citep">(Chatterji et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib8" title="">2025</a>; Tamkin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib21" title="">2024</a>; Appel et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib3" title="">2025</a>; Acemoglu, <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib1" title="">2025</a>; Bick et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib4" title="">2024</a>)</cite>. However, historical evidence from technological shifts—such as electricity, airplanes, and computers—shows that the transition from invention to economy-wide permeation often takes years or even decades, requiring regulatory, cultural, and procedural changes <cite class="ltx_cite ltx_citemacro_citep">(David, <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib10" title="">1990</a>; Brynjolfsson &amp; Hitt, <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib5" title="">2000</a>; Brynjolfsson et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib6" title="">2017</a>; Dwivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib11" title="">2021</a>; Solow, <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib20" title="">1987</a>)</cite>. Therefore, while informative when available, these methods are lagging indicators of AI impacts. We consider an alternate method for understanding the potential economic impacts of AI: directly measuring AI model capabilities. AI capability evaluations can provide clearer, more directly attributable evidence about model abilities, allowing us to assess economic relevance ahead of widespread adoption.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="270" id="S1.F1.g1" src="exampletasksscreenshot.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example GDPval tasks from full set</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">Our paper introduces the first version of GDPval, a benchmark evaluating AI model performance on real-world economically valuable tasks. GDPval covers the top 9 sectors contributing to U.S. GDP (Gross Domestic Product), with at least 30 tasks per occupation in the full set (and 5 tasks per occupation in the gold subset), across 44 occupations. Each task is constructed based on actual work product created by an expert professional. Given the complexity of automatically grading these tasks, our primary evaluation metric is head-to-head human expert comparison. We also provide an experimental automated grader service for the 220 open-sourced gold subset of tasks. Future GDPval iterations will incorporate greater breadth, realism, interactivity, and contextual nuance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">The initial version of GDPval offers several advantages over existing AI model evaluations:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Realism</span>: Unlike AI benchmarks in the style of an academic test that focus on reasoning difficulty (e.g., <cite class="ltx_cite ltx_citemacro_cite">Phan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib18" title="">2025</a>); Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib14" title="">2020</a>); Rein et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib19" title="">2023</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib15" title="">2023</a>)</cite>), tasks are based on actual work product from industry experts, validated through multiple rounds and review, and tied to time and cost required for completion.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Representative breadth:</span> Unlike AI evaluations focused on specific domains like software engineering (e.g., <cite class="ltx_cite ltx_citemacro_cite">Miserendino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib16" title="">2025</a>)</cite>), the GDPval full set covers 1,320 tasks across 44 occupations, sourced to cover the majority of Work Activities tracked by O*NET for each occupation <cite class="ltx_cite ltx_citemacro_cite">U.S. Department of Labor, Employment and Training
Administration (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib24" title="">2024</a>)</cite>. This top-down approach allows for representativeness of tasks across occupations. We also build on production AI usage analyses (e.g., <cite class="ltx_cite ltx_citemacro_cite">Tamkin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib21" title="">2024</a>); Chatterji et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib8" title="">2025</a>); Appel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib3" title="">2025</a>)</cite>) to cover areas where model adoption is still emerging.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Computer use and multi-modality</span>: Tasks require manipulating a variety of formats (e.g., CAD design files, photos, video, audio, social media posts, diagrams, slide decks, spreadsheets, and customer support conversations). Each task also requires parsing through up to 17 reference files in the gold subset, and 38 in the full set.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Subjectivity</span>: In addition to correctness, expert graders often consider subjective factors such as structure, style, format, aesthetics, and relevance. Our dataset also therefore serves as a helpful testbed to assess automated grader performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">No “upper limit”</span>: Unlike metrics that could saturate quickly, our primary metric is win rate, which allows for continuous evaluation. Currently, we compare model outputs against a human expert baseline, but we could replace our baseline with increasingly strong models over time and keep evaluating.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Long-horizon difficulty</span>: Tasks require an average of 7 hours of work for an expert professional to complete. On the high end, tasks span up to multiple weeks of work.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Task Creation</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p">We first identify the sectors that contribute most to U.S. GDP, then source tasks drawn from the highest-earning knowledge work occupations within those sectors.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Prioritizing occupations</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p">GDPval covers tasks from 9 sectors and 44 occupations that collectively earn $3T annually. We detail below the methodology behind our initial version.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p">To choose the initial occupations, we:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Selected sectors that contribute over 5% to US GDP</span> as determined by Q2 2024 Value Added by Industry as a Percentage of Gross Domestic Product (see <cite class="ltx_cite ltx_citemacro_cite">Federal Reserve Bank of St. Louis (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib13" title="">2025</a>)</cite>). These 9 sectors are shown in Table 1.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Selected the 5 occupations <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">1</span></span><span class="ltx_text ltx_font_medium">We assigned occupations to sectors by using the 2023 BLS National Employment Matrix from </span><cite class="ltx_cite ltx_citemacro_cite">U.S. Bureau of Labor
Statistics <span class="ltx_text ltx_font_medium">(</span><a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib22" title="">2025a</a><span class="ltx_text ltx_font_medium">)</span></cite><span class="ltx_text ltx_font_medium"> to map occupations to sectors by identifying the sector with the highest employment for each occupation. For more detail, see </span><a class="ltx_ref ltx_font_medium" href="https://arxiv.org/html/2510.04374v1#A1.SS7" title="A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.7</span></a><span class="ltx_text ltx_font_medium">.</span></span></span></span> within each sector that contribute most to total wages and compensation and are predominantly digital.</span> We took a task-based approach to determining if an occupation should be classified as “predominantly digital.” Specifically, we identified all tasks for an occupation from O*NET, a database of occupational data, definitions and tasks from the U.S. Department of Labor. Similar to <cite class="ltx_cite ltx_citemacro_cite">Eloundou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib12" title="">2023</a>)</cite>, we prompted GPT-4o to classify each task as digital or non-digital, and then classified the overall occupation as digital if at least 60% of its component tasks were digital. To calculate this percentage, we weighted tasks by the “relevance,” “importance,” and “frequency” scores for each task reported in O*NET Task Ratings.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p">We further validated the representativeness of our digital tasks measure by benchmarking it against the <cite class="ltx_cite ltx_citemacro_cite">Acemoglu &amp; Autor (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib2" title="">2011</a>)</cite> task content framework. The correlations we observe—digital tasks increasing with non-routine cognitive content and decreasing with routine and manual content—demonstrate alignment with established economic measures of work, as per <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7.SSS1" title="A.7.1 Validating the Digital Tasks Measure ‣ A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.7.1</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p">For wage and occupation data, we used O*NET’s May 2024 national employment and wage estimates to calculate total wages for 831 occupations (<cite class="ltx_cite ltx_citemacro_cite">U.S. Bureau of Labor
Statistics (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib23" title="">2025b</a>)</cite>) and further detailed in  <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS7" title="A.7 Further Methodological Details on Selecting Occupations ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.7</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="312" id="S2.F2.g1" src="assets/occs_0922_f.png" width="548"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>GDPval includes real-world work from 44 occupations.</figcaption>
</figure>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"><span class="ltx_text ltx_align_left ltx_font_bold">Sector</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"><span class="ltx_text ltx_align_left ltx_font_bold">% GDP</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;"><span class="ltx_text ltx_align_left ltx_font_bold">Top Occupations and Total Compensation (in Billions USD)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Real Estate and Rental and Leasing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">13.8%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Property/RE/Community Association Managers — $24.54B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Counter and Rental Clerks — $17.42B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Real Estate Sales Agents — $13.53B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Real Estate Brokers — $4.55B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Concierges — $1.80B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Manufacturing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">10.0%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">First-Line Supervisors of Production and Operating Workers — $51.07B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Buyers and Purchasing Agents — $39.79B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Shipping, Receiving, and Inventory Clerks — $38.50B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Industrial Engineers — $37.79B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Mechanical Engineers — $31.57B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Professional, Scientific, and Technical Services</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">8.1%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Software Developers — $239.18B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Lawyers — $136.66B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Accountants and Auditors — $135.44B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Computer and Information Systems Managers — $121.44B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Project Management Specialists — $108.77B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Government</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">11.3%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Compliance Officers — $33.80B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Administrative Services Managers — $32.03B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Child, Family, and School Social Workers — $24.10B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">First-Line Supervisors of Police and Detectives — $17.00B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Recreation Workers — $11.51B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Health Care and Social Assistance</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">7.6%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Registered Nurses — $323.05B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">First-Line Supervisors of Office/Admin Support — $107.02B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Medical &amp; Health Services Managers — $77.93B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Nurse Practitioners — $40.58B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Medical Secretaries &amp; Admin Assistants — $37.87B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Finance and Insurance</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">7.4%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Financial Managers — $147.74B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Customer Service Representatives — $123.70B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Securities, Commodities, and Financial Services Sales Agents — $52.14B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Personal Financial Advisors — $43.33B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Financial and Investment Analysts — $39.67B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Retail Trade</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">6.3%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">General &amp; Operations Managers — $477.16B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">1st-Line Supervisors of Retail Sales Workers — $58.27B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Pharmacists — $45.12B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Private Detectives &amp; Investigators — $2.39B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Wholesale Trade</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">5.8%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Sales Reps, Wholesale &amp; Mfg (Except Tech/Scientific) — $103.21B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Sales Managers — $97.16B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Sales Reps, Wholesale &amp; Mfg (Tech/Scientific) — $33.66B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">1st-Line Supervisors of Non-Retail Sales Workers — $21.43B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Order Clerks — $3.86B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;">Information</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;">5.4%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Producers &amp; Directors — $16.60B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Editors — $8.18B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">News Analysts, Reporters, and Journalists — $4.41B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Audio &amp; Video Technicians — $4.30B</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:85.4pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:42.7pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding:0.6pt 4.0pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:284.5pt;">Film &amp; Video Editors — $2.41B</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Sectors, their value added as a percentage of U.S. GDP (Q2 2024), with representative top occupations and total compensation in billions (USD).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Expert Recruitment</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p">We recruited expert industry professionals to create realistic tasks based on their professional work experience. Experts were required to have a minimum of 4 years of professional experience in their occupation and a strong resume with a demonstrated history of professional recognition, promotion, and management responsibilities. The average expert had 14 years of experience. We further required experts to pass a video interview, a background check, a training and a quiz to participate in the project. Experts were well compensated for their time and experience. Some of the prior employers of our industry experts include: Accenture, Aetna, Apple, AXA Advisors, Bank of America, Barclays, BBC News, Boeing, Budget Rent a Car, Capital One, Centers for Disease Control and Prevention, Citigroup, Condé Nast, CVS Pharmacy, U.S. Department of Defense, Disney, Douglas Elliman, E*TRADE, Federal Trade Commission, General Electric, Goldman Sachs, Google, Guggenheim Partners, HBO, IBM, JPMorgan Chase, Johnson &amp; Johnson, Kmart, Kirkland &amp; Ellis LLP, LinkedIn, Lockheed Martin, Macy’s, Massachusetts General Hospital, Meta, Microsoft, Morgan Stanley, National Park Service, NFL Network, Oracle, Paul, Weiss, Rifkind, Wharton &amp; Garrison LLP, Prudential, PwC, Raytheon, Sally Beauty, Samsung, SAP, Scientific American, Sotheby’s, Telegraph Media Group, Thermo Fisher Scientific, TIME, Twilio, U.S. Department of Justice, United States Air Force, United States Postal Service, Walgreens, Wells Fargo, White &amp; Case LLP, and Whole Foods.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Task Creation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p">Each GDPval task consists of two primary components: a request (often with reference files) and a deliverable (work product). Experts classified their requests against O*NET occupational tasks for their occupation to ensure broad and representative coverage across tasks <cite class="ltx_cite ltx_citemacro_citep">(U.S. Bureau of Labor
Statistics, <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib22" title="">2025a</a>)</cite>. More details on task characteristics can be found in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS4" title="A.4 Additional Task Characteristics ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.4</span></a>. To assess task quality, we asked occupational experts to rate each task on its difficulty, representativeness, time to complete, and overall quality against real-world standards for their occupation. Each task’s dollar value was estimated by multiplying the average estimated completion time by median hourly wages for the corresponding occupation from OEWS data <cite class="ltx_cite ltx_citemacro_citep">(U.S. Bureau of Labor
Statistics, <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib23" title="">2025b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Task Quality Control Pipeline</h3>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S2.F3.g1" src="assets/reviewdiagram_0923.png" width="494"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Tasks undergo multiple rounds of review to ensure realism and quality.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p">All 1,320 tasks in the full GDPval set went through an iterative review pipeline involving both automated model-based screening and multiple stages of human expert review. Each task received an average of five human reviews (with a minimum of three reviews).</p>
</div>
<figure class="ltx_figure" id="S2.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="465" id="S2.F4.sf1.g1" src="assets/no_logo_grading.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Pairwise Grading Setup</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="480" id="S2.F4.sf2.g1" src="agreement_with_humans.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Agreement with Humans</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>GDPval uses pairwise expert comparisons for grading. We also create an experimental automated grader. We find that automated grader agreement is within 5% of human inter-rater agreement on the GDPval gold subset.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS4.p2">
<p class="ltx_p">Across all stages of review, experts provided detailed comments, and tasks were iteratively revised before subsequent reviews to enhance quality and representativeness, as detailed in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS5" title="A.5 Further Detail on Task Quality Control ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.5</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Human Expert Grading and Automated Grading</h3>
<div class="ltx_para ltx_noindent" id="S2.SS5.p1">
<p class="ltx_p">To grade the 220 open-sourced gold subset, we conducted blinded expert pairwise comparisons, where experts in the relevant occupation were presented with a request and reference files and asked to rank two or more unlabeled work deliverables.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p2">
<p class="ltx_p">On average, grading each comparison for the gold subset took over an hour. Additional occupational experts were sourced to grade human and model deliverables. Experts provided detailed justifications for their choices and rankings, which enabled us to compute our headline win rates for various models compared to the human expert completion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p3">
<p class="ltx_p">For the gold subset, we trained an experimental grading model to perform pairwise comparisons in the style of industry professional experts. Although limited, the automated grader is faster and cheaper than expert grading, and achieves 66% agreement with human expert graders, only 5% below human expert inter-rating agreement of 71%. Further detail is in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6" title="A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.6</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Headline results</h3>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="329" id="S3.F5.g1" src="assets/gdpval_winrates.png" width="502"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>On human pairwise comparisons, models are beginning to approach parity with industry experts on the GDPval gold subset.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="S3.F6.g1" src="assets/gdpval_openai_frontier_model_performance_over_time.png" width="384"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Performance of OpenAI frontier models increased roughly linearly over time on the GDPval gold subset.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p">We evaluated GPT-4o, o4-mini, o3, GPT-5, Claude Opus 4.1, Gemini 2.5 Pro, and Grok 4 using blind pairwise comparisons by professional industry experts <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We aimed to keep comparisons as blind as possible, but model samples may still have been identifiable due to stylistic differences. OpenAI outputs often used em dashes, Claude outputs frequently adopted first-person phrasing, and Grok occasionally referred to itself as Grok. Although filenames were scrubbed of model identifiers, to preserve sample identity, we did not alter style or content, so experts may still have been able to infer model origins. We sampled Claude via the UI to enable the maximum GDPval-relevant features. For example, for Claude, we wanted to evaluate its ‘Upgraded file creation and analysis’ feature (https://www.anthropic.com/news/create-files). For the OpenAI models, we enabled the web search tool and the code interpreter tool, with background sampling. We also preinstalled several libraries not available in the base image, see <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS4" title="A.6.4 automated grader Packages ‣ A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.6.4</span></a>. For plots shown, we sampled each model 3 times for each prompt, and then had 3 different human graders grade each sample (yielding 9 comparisons per prompt, per model, across 220 tasks).</span></span></span>. Claude Opus 4.1 was the best performing model on the GDPval gold subset, excelling in particular on aesthetics (e.g., document formatting, slide layout), while GPT-5 excelled in particular on accuracy (e.g., carefully following instructions, performing correct calculations) as per <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.F8" title="In 3.3 Model strengths and weaknesses ‣ 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>. This distinction is also shown in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS4" title="A.2.4 Win rates by deliverable ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.2.4</span></a>, where GPT-5 performs better on pure text, while Claude performs better on file types like .pdf, .xslx, and .ppt, demonstrating better visual and aesthetic abilities <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We caveat also that the occupations and task types covered by text tend to be different than those that involve multi-modal</span></span></span>. In <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.F5" title="In 3.1 Headline results ‣ 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, on the GDPval gold subset, 47.6% of deliverables by Claude Opus 4.1 were graded as better than (wins) or as good as (ties) the human deliverable. Model deliverables outperformed or matched expert humans’ deliverables in just over half the tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Speed and cost comparison</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">We analyzed several scenarios to understand the potential speed and cost savings ratio of frontier models on the GDPval gold subset tasks in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS1" title="A.2.1 Speed and Cost Analysis, continued ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.2.1</span></a><span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We were not able to obtain cost estimates for Claude, Gemini, and Grok.</span></span></span>. In the scenarios analyzed, incorporating frontier AI models into expert workflows showed the potential to save time and money relative to unaided experts. Fig <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.F7" title="Figure 7 ‣ 3.2 Speed and cost comparison ‣ 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">7</span></a> summarizes expected savings under a “try using the model and if still unsatisfactory, fix it yourself” setup. Here, an expert human samples from a model, reviews outputs, and if unsatisfactory, resamples and repeats. If no satisfactory output is obtained, the human completes the task themselves. Under this setup, as well as other setups (e.g., directly using model outputs, trying the model just once before doing work directly), model assistance can potentially save the expert time and money.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="324" id="S3.F7.g1" src="assets/speed_cost_mult_no_v0.png" width="302"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>In the scenarios we analyze, models show the potential to save time and money by coupling AI assistance with expert human oversight. Here, we show speed and cost savings from a “try <math alttext="n" class="ltx_Math" display="inline" id="S3.F7.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> times, and if still unsatisfactory, fix it yourself” approach as detailed in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS1" title="A.2.1 Speed and Cost Analysis, continued ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.2.1</span></a>.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model strengths and weaknesses</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p">We built a clustering pipeline to analyze why experts preferred or rejected GPT-5 high, Claude Opus 4.1, Gemini 2.5 Pro, and Grok 4 deliverables as shown in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.F8" title="In 3.3 Model strengths and weaknesses ‣ 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Samples were clustered using expert justifications; labels were mutually exclusive and left blank when the rationale was unclear.</span></span></span> Claude, Grok, and Gemini most often lost due to instruction-following failures, while GPT-5 high lost mainly from formatting errors and had the fewest instruction-following issues. Gemini and Grok frequently promised but failed to provide deliverables, ignored reference data, or used the wrong format. GPT-5 and Grok showed the fewest accuracy errors, though all models sometimes hallucinated data or miscalculated.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F8.g1" src="assets/failure_modes_by_model.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Across models, experts most often preferred the human deliverable because models failed to fully follow instructions on GDPval tasks.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Increasing reasoning effort and scaffolding</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p">To understand the impact of reasoning effort on model performance, we ran GDPval on the o3 and GPT-5 models at low, medium, and high reasoning effort. We found that additional reasoning effort improved performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p">We were also interested in measuring how easily we could improve model capabilities with prompts. For example, many of the observed GPT-5 failure modes were due to obvious formatting errors. We created a prompt which encouraged GPT-5 to rigorously check deliverables for correctness, check layouts by rendering files as images, avoid nonstandard unicode characters, and avoid excess verbosity. The prompt applies generally to multimodal economic tasks and is not overfit to any given question (see <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS3" title="A.3 Additional Detail on Prompt-Tuning ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.3</span></a> for details). We also improved agent scaffolding by enabling GET requests in the container and performing best-of-N sampling with N=4 and a GPT-5 judge.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p">Prompting fully eliminated black-square artifacts from GPT-5 responses, which previously affected over half of generated PDFs, and reduced egregious formatting errors in PowerPoint files from 86% to 64%. This can be partially attributed to a sharp increase in agents using their multi-modal capabilities to inspect deliverables (15% <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS4.p3.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> 97%). Prompting also improved human preference win rates by 5 percentage points in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S3.F9.sf2" title="Figure 9(b) ‣ Figure 9 ‣ 3.4 Increasing reasoning effort and scaffolding ‣ 3 Experiments and Results ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">9(b)</span></a>. These easy performance gains suggest there are paths to agent improvement on GDPval tasks by training or scaffolding them to be more thorough and take full advantage of their multimodal capabilities.</p>
</div>
<figure class="ltx_figure" id="S3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="569" id="S3.F9.sf1.g1" src="assets/reasoning_effort.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Reasoning effort experiment</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="594" id="S3.F9.sf2.g1" src="assets/prompt_tuning.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Prompt tuning experiment</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Model performance improves predictably with increasing reasoning effort. Prompt-tuning and scaffolding improvements also increase GPT-5 performance.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Open-sourcing</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p">We open-source the prompts and reference files in our 220-task gold subset. While human expert comparison is still our recommended method of grading, we make an experimental automated grader publicly available at <a class="ltx_ref ltx_href" href="evals.openai.com" title="">evals.openai.com</a>. Please note that the tasks in the open sourced set have been scrubbed of information that could be used to identify the expert who wrote the task. We also note that, as a result of limitations with our automated grader, we don’t provide automated grading results for all tasks in the gold subset. Further disclaimers about the open source gold subset are in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS1.SSS3" title="A.1.3 Third-party references disclosure ‣ A.1 Disclosures ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.1.3</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Dataset size:</h5>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p">The GDPval full set currently consists of only 44 occupations and 30 total tasks per occupation. It is therefore a limited, initial cut of knowledge work tasks, not a comprehensive evaluation of all possible occupational tasks. We are expanding the dataset size.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Focus on self-contained knowledge work:</h5>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p">Tasks in the initial version of GDPval are oriented around knowledge work that can be performed on a computer, particularly around digital deliverables. Manual labor and physical tasks are not included in the current version. Moreover, tasks that involve extensive tacit knowledge, access to personally identifiable information, use of proprietary software tools, or communication between individuals are out of scope for the current evaluation. We aim to build on this in future versions of the evaluation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Tasks are precisely-specified and one-shot, not interactive:</h5>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p">For GDPval, we provide the full context of the task in the prompt, but in real life it often takes effort to figure out the full context of a task and understand what to work on. We are working on improvements to GDPval that involve more interactivity and contextual realism. In the meantime, the experiment in the “Under-contextualized GDPval” section (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS2.SSS7" title="A.2.7 Under-contextualized GDPval ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.2.7</span></a>) demonstrates how model performance degrades with less context.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Grader performance:</h5>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p">Our current automated grader has a number of limitations compared to human expert graders. More details about the automated grader are available in the <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.SS6.SSS2" title="A.6.2 Automated Grader Correlation Results ‣ A.6 Automated Grader Details ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">A.6.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Cost:</h5>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p">Constructing and running our evaluation is expensive, particularly with industry expert graders. For this reason, we make an automated grader proxy available, but do not consider it a full substitute for industry expert graders.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p">In GDPval, we contribute the following:</p>
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Dataset</span>: We create a new evaluation dataset (GDPval) measuring real-world, economically valuable tasks.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Capability benchmarking:</span> We analyze quality, speed and cost of deliverables across human industry experts and frontier AI models.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Experiments:</span> We test how results shift with differing reasoning effort, prompting, scaffolding, and context.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Open-sourcing:</span> We open-source 220 tasks as part of our gold subset which includes prompts and reference files.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="S6.I1.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Automated grader:</span> We release an automated grader to improve accessibility of grading at <a class="ltx_ref ltx_href" href="evals.openai.com" title="">evals.openai.com</a>.</p>
</div>
</li>
</ol>
<p class="ltx_p">We hope this work contributes to the science of tracking model progress, so that we have better data to assess the social impacts of AI models.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p">We thank Abhishek Bhardwaj, Addea Gupta, AJ Ostrow, Aleksander Madry, Alexander Wei, Ally Bennett, Becky Waite, Ben Gaffney, Brad Lightcap, Casey Chu, Cassandra Duchan Solis, Charlotte Cole, Dane Stuckey, Eric Wallace, Erik Ritter, Evan Mays, Fidji Simo, Gideon Myles, Hannah Wong, Isa Fulford, Jakub Pachocki, James Lennon, Jared Pochtar, Jason Kwon, Jordan Frand, Julie Steele, Justin Wang, Kai Chen, Karthik Rangarajan, Kevin Liu, Larry Summers, Leo Liu, Leon Maksin, Leyton Ho, Lindsay McCallum, Livvy Pierce, Manoli Liodakis, Mark Chen, Max Schwarzer, Miles Palley, Miles Wang, Nakul Khanna, Nat McAleese, Natalie Kim, Nicholas Carlini, Nick Otis, Nick Ryder, Noam Brown, Noel Bundick, Paul Radulovic, Phillip Guo, Prashanth R, Rachel Brown, Raoul de Liedekerke, Robert Rotsted, Ronnie Chatterji, Ryan Kaufman, Ryan Rotsted, Sam Altman, Sam Bowman, Sherwin Wu, Tom Cunningham, Tom Stasi, Tony Song, Trevor Creech, Wenda Zhou, Wenlei Xie, Wyatt Thompson, and Yara Khakbaz for discussion, assistance, and review. We are also grateful to our vendor partners for their collaboration and support throughout this research, and extend a special thank you to the industry experts who contributed their time and expertise to GDPval, without whom this work would not have been possible.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acemoglu (2025)</span>
<span class="ltx_bibblock">
Daron Acemoglu.

</span>
<span class="ltx_bibblock">The simple macroeconomics of ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Economic Policy</em>, 40(121):13–58, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acemoglu &amp; Autor (2011)</span>
<span class="ltx_bibblock">
Daron Acemoglu and David Autor.

</span>
<span class="ltx_bibblock">Skills, tasks and technologies: Implications for employment and
earnings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Handbook of labor economics</em>, volume 4, pp.  1043–1171.
Elsevier, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Appel et al. (2025)</span>
<span class="ltx_bibblock">
Ruth Appel, Peter McCrory, Alex Tamkin, Michael Stern, Miles McCain, and Tyler
Neylon.

</span>
<span class="ltx_bibblock">Anthropic economic index report: Uneven geographic and enterprise ai
adoption.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Anthropic Research</em>, 2025.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/research/anthropic-economic-index-september-2025-report" title="">https://www.anthropic.com/research/anthropic-economic-index-september-2025-report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bick et al. (2024)</span>
<span class="ltx_bibblock">
Alexander Bick, Adam Blandin, and David J Deming.

</span>
<span class="ltx_bibblock">The rapid adoption of generative ai.

</span>
<span class="ltx_bibblock">Technical report, National Bureau of Economic Research, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brynjolfsson &amp; Hitt (2000)</span>
<span class="ltx_bibblock">
Erik Brynjolfsson and Lorin M. Hitt.

</span>
<span class="ltx_bibblock">Beyond computation: Information technology, organizational
transformation and business performance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of Economic Perspectives</em>, 14(4):23–48, 2000.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1257/jep.14.4.23</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aeaweb.org/articles?id=10.1257/jep.14.4.23" title="">https://www.aeaweb.org/articles?id=10.1257/jep.14.4.23</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brynjolfsson et al. (2017)</span>
<span class="ltx_bibblock">
Erik Brynjolfsson, Daniel Rock, and Chad Syverson.

</span>
<span class="ltx_bibblock">Artificial intelligence and the modern productivity paradox: A clash
of expectations and statistics.

</span>
<span class="ltx_bibblock">Working Paper 24001, National Bureau of Economic Research, November
2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.nber.org/papers/w24001" title="">http://www.nber.org/papers/w24001</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brynjolfsson et al. (2025)</span>
<span class="ltx_bibblock">
Erik Brynjolfsson, Danielle Li, and Lindsey Raymond.

</span>
<span class="ltx_bibblock">Generative ai at work.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Quarterly Journal of Economics</em>, 140(2):889–942, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chatterji et al. (2025)</span>
<span class="ltx_bibblock">
Aaron Chatterji, Thomas Cunningham, David J Deming, Zoe Hitzig, Christopher
Ong, Carl Yan Shan, and Kevin Wadman.

</span>
<span class="ltx_bibblock">How people use chatgpt.

</span>
<span class="ltx_bibblock">Working Paper 34255, National Bureau of Economic Research, September
2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.nber.org/papers/w34255" title="">http://www.nber.org/papers/w34255</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2025)</span>
<span class="ltx_bibblock">
Wilbur Xinyuan Chen, Suraj Srinivasan, and Saleh Zakerinia.

</span>
<span class="ltx_bibblock">Displacement or complementarity?: The labor market impact of
generative ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Harvard Business School</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">David (1990)</span>
<span class="ltx_bibblock">
Paul A. David.

</span>
<span class="ltx_bibblock">The dynamo and the computer: An historical perspective on the modern
productivity paradox.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">American Economic Review</em>, 80(2):355–361,
1990.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://econpapers.repec.org/RePEc:aea:aecrev:v:80:y:1990:i:2:p:355-61" title="">https://econpapers.repec.org/RePEc:aea:aecrev:v:80:y:1990:i:2:p:355-61</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwivedi et al. (2021)</span>
<span class="ltx_bibblock">
Yogesh K Dwivedi, Laurie Hughes, Elvira Ismagilova, Gert Aarts, Crispin Coombs,
Tom Crick, Yanqing Duan, Rohita Dwivedi, John Edwards, Aled Eirug, et al.

</span>
<span class="ltx_bibblock">Artificial intelligence (ai): Multidisciplinary perspectives on
emerging challenges, opportunities, and agenda for research, practice and
policy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">International journal of information management</em>, 57:101994, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eloundou et al. (2023)</span>
<span class="ltx_bibblock">
Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock.

</span>
<span class="ltx_bibblock">Gpts are gpts: An early look at the labor market impact potential of
large language models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.10130" title="">https://arxiv.org/abs/2303.10130</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Federal Reserve Bank of St. Louis (2025)</span>
<span class="ltx_bibblock">
Federal Reserve Bank of St. Louis.

</span>
<span class="ltx_bibblock">Value added by industry as a percentage of gross domestic product.

</span>
<span class="ltx_bibblock">FRED Release Tables, 2025.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://fred.stlouisfed.org/release/tables?rid=331&amp;eid=211" title="">https://fred.stlouisfed.org/release/tables?rid=331&amp;eid=211</a>.

</span>
<span class="ltx_bibblock">Accessed: 2025-09-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2009.03300</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2009.03300" title="">https://arxiv.org/abs/2009.03300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu,
Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan
Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.

</span>
<span class="ltx_bibblock">Agentbench: Evaluating llms as agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.03688</em>, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2308.03688</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miserendino et al. (2025)</span>
<span class="ltx_bibblock">
Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke.

</span>
<span class="ltx_bibblock">Swe-lancer: Can frontier LLMs earn $1 million from real-world
freelance software engineering?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2502.12115</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panickssery et al. (2024)</span>
<span class="ltx_bibblock">
Arjun Panickssery, Samuel R. Bowman, and Shi Feng.

</span>
<span class="ltx_bibblock">Llm evaluators recognize and favor their own generations, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.13076" title="">https://arxiv.org/abs/2404.13076</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phan et al. (2025)</span>
<span class="ltx_bibblock">
Long Phan et al.

</span>
<span class="ltx_bibblock">Humanity’s last exam.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.14249</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rein et al. (2023)</span>
<span class="ltx_bibblock">
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe
Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman.

</span>
<span class="ltx_bibblock">GPQA: A graduate-level google-proof q&amp;a benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.12022</em>, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2311.12022</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.12022" title="">https://arxiv.org/abs/2311.12022</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solow (1987)</span>
<span class="ltx_bibblock">
Robert M. Solow.

</span>
<span class="ltx_bibblock">We’d better watch out.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">New York Times Book Review</em>, pp.  36, July 1987.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.standupeconomist.com/pdf/misc/solow-computer-productivity.pdf" title="">https://www.standupeconomist.com/pdf/misc/solow-computer-productivity.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamkin et al. (2024)</span>
<span class="ltx_bibblock">
Alex Tamkin, Miles McCain, Kunal Handa, Esin Durmus, Liane Lovitt, Ankur Rathi,
Saffron Huang, Alfred Mountfield, Jerry Hong, Stuart Ritchie, Michael Stern,
Brian Clarke, Landon Goldberg, Theodore R. Sumers, Jared Mueller, William
McEachen, Wes Mitchell, Shan Carter, Jack Clark, Jared Kaplan, and Deep
Ganguli.

</span>
<span class="ltx_bibblock">Clio: Privacy-preserving insights into real-world ai use.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2412.13678</em>, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2412.13678</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2412.13678" title="">https://arxiv.org/abs/2412.13678</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">U.S. Bureau of Labor
Statistics (2025a)</span>
<span class="ltx_bibblock">
U.S. Bureau of Labor Statistics.

</span>
<span class="ltx_bibblock">Occupational outlook – occupational data.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bls.gov/emp/data/occupational-data.htm" title="">https://www.bls.gov/emp/data/occupational-data.htm</a>,
2025a.

</span>
<span class="ltx_bibblock">Accessed: 2025-09-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">U.S. Bureau of Labor
Statistics (2025b)</span>
<span class="ltx_bibblock">
U.S. Bureau of Labor Statistics.

</span>
<span class="ltx_bibblock">Occupational employment and wage statistics: May 2024 national
tables.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bls.gov/oes/tables.htm" title="">https://www.bls.gov/oes/tables.htm</a>, 2025b.

</span>
<span class="ltx_bibblock">Data reference May 2024; accessed: 2025-09-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">U.S. Department of Labor, Employment and Training
Administration (2024)</span>
<span class="ltx_bibblock">
U.S. Department of Labor, Employment and Training Administration.

</span>
<span class="ltx_bibblock">Work activities - o⁢net 28.3 data dictionary.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.onetcenter.org/dictionary/28.3/excel/task_ratings.html" title="">https://www.onetcenter.org/dictionary/28.3/excel/task_ratings.html</a>,
2024.

</span>
<span class="ltx_bibblock">Accessed: 2025-04-20.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Disclosures</h3>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>AI disclosure</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS1.p1">
<p class="ltx_p">We used AI models to help with our literature review and with tweaking language in the paper. We also used AI coding assistants as part of our regular engineering workflows (e.g., to help find and fix bugs).</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2 </span>Sensitive content and political content disclosure</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS2.p1">
<p class="ltx_p">Some tasks in GDPval include NSFW content, including themes such as sex, alcohol, vulgar language, and political content. We chose to keep these tasks as they reflect real themes addressed in various occupations (e.g., film, literature, law, politics). We do not endorse the particular actions or views in any of the content.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.3 </span>Third-party references disclosure</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS3.p1">
<p class="ltx_p">GDPval contains limited references to third-party brands and trademarks solely for research and evaluation purposes. No affiliation or endorsement is intended or implied. All trademarks are the property of their respective owners. Some images and videos in this dataset feature AI-generated individuals and real people who have provided permission. Names and identifying references to private individuals in GDPval are fictitious. Any resemblance to actual persons or entities is purely coincidental.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Additional Detail on Experimental Results</h3>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Speed and Cost Analysis, continued</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS1.p1">
<p class="ltx_p">We use the following definitions:</p>
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p">Human expert professional completion time <math alttext="H_{T}" class="ltx_Math" display="inline" id="A1.I1.i1.p1.m1" intent=":literal"><semantics><msub><mi>H</mi><mi>T</mi></msub><annotation encoding="application/x-tex">H_{T}</annotation></semantics></math> is the time taken by a human expert professional to complete a task, based on validated self-reported time to complete<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>During submission, experts self-reported the real-world time required to complete each task. Multiple occupational reviewers independently validated these times, correcting errors. Because times were self-reported, it is possible that experts under-estimated or over-estimated time taken</span></span></span>. To calculate human expert professional completion cost <math alttext="H_{C}" class="ltx_Math" display="inline" id="A1.I1.i1.p1.m2" intent=":literal"><semantics><msub><mi>H</mi><mi>C</mi></msub><annotation encoding="application/x-tex">H_{C}</annotation></semantics></math>, we multiplied the reported task completion hours per occupation by the median hourly wage for each occupation from the <cite class="ltx_cite ltx_citemacro_cite">U.S. Bureau of Labor
Statistics (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib23" title="">2025b</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Because our experts were recruited specifically for being highly experienced in their field, these wage estimates likely underestimate their true market cost.</span></span></span>. On average, on our 220 gold subset <math alttext="H_{T}=404" class="ltx_Math" display="inline" id="A1.I1.i1.p1.m3" intent=":literal"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>=</mo><mn>404</mn></mrow><annotation encoding="application/x-tex">H_{T}=404</annotation></semantics></math> minutes and <math alttext="H_{C}=\mathdollar 361" class="ltx_Math" display="inline" id="A1.I1.i1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>H</mi><mi>C</mi></msub><mo>=</mo><mrow><mi mathvariant="normal">$</mi><mo lspace="0em" rspace="0em">​</mo><mn>361</mn></mrow></mrow><annotation encoding="application/x-tex">H_{C}=\mathdollar 361</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p">Human expert professional review time <math alttext="R_{T}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m1" intent=":literal"><semantics><msub><mi>R</mi><mi>T</mi></msub><annotation encoding="application/x-tex">R_{T}</annotation></semantics></math> is an estimate of the time taken to assess a model deliverable by a human expert grader. We observe this from our task monitoring software, averaging the time taken to grade for the first time each human expert was asked to grade that question. On average, <math alttext="R_{T}=109" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m2" intent=":literal"><semantics><mrow><msub><mi>R</mi><mi>T</mi></msub><mo>=</mo><mn>109</mn></mrow><annotation encoding="application/x-tex">R_{T}=109</annotation></semantics></math> minutes, and associated human expert professional review cost <math alttext="R_{C}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m3" intent=":literal"><semantics><msub><mi>R</mi><mi>C</mi></msub><annotation encoding="application/x-tex">R_{C}</annotation></semantics></math> is on average $86, where <math alttext="R_{C}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m4" intent=":literal"><semantics><msub><mi>R</mi><mi>C</mi></msub><annotation encoding="application/x-tex">R_{C}</annotation></semantics></math> is again calculated based on time taken multiplied by median wage data.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p">Model completion time <math alttext="M_{T}" class="ltx_Math" display="inline" id="A1.I1.i3.p1.m1" intent=":literal"><semantics><msub><mi>M</mi><mi>T</mi></msub><annotation encoding="application/x-tex">M_{T}</annotation></semantics></math> is the time taken for the model to complete a deliverable and <math alttext="M_{C}" class="ltx_Math" display="inline" id="A1.I1.i3.p1.m2" intent=":literal"><semantics><msub><mi>M</mi><mi>C</mi></msub><annotation encoding="application/x-tex">M_{C}</annotation></semantics></math> is the associated completion cost, based on empirical API speed and cost for the model to complete the deliverable when given a prompt <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>For each task, we collected three API completions per model and averaged the observed response times recorded in the API metadata. We also recorded the average invoiced cost per task.</span></span></span>.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i4.p1">
<p class="ltx_p">Model win rate <math alttext="w" class="ltx_Math" display="inline" id="A1.I1.i4.p1.m1" intent=":literal"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> is how often the model deliverable is rated better than the human deliverable by the human expert grader.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS1.p2">
<p class="ltx_p">We then calculate the following ratios:</p>
<ol class="ltx_enumerate" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Naive ratio:</span> To measure the ratio of human deliverable versus model deliverable, without accounting for any quality differences or implementation times, we simply divide the average task completion time for a human by the average sampling time for a model: <math alttext="H_{T}/M_{T}" class="ltx_Math" display="inline" id="A1.I2.i1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>/</mo><msub><mi>M</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">H_{T}/M_{T}</annotation></semantics></math>, and analogously for cost: <math alttext="H_{C}/M_{C}" class="ltx_Math" display="inline" id="A1.I2.i1.p1.m2" intent=":literal"><semantics><mrow><msub><mi>H</mi><mi>C</mi></msub><mo>/</mo><msub><mi>M</mi><mi>C</mi></msub></mrow><annotation encoding="application/x-tex">H_{C}/M_{C}</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="A1.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Try <math alttext="1" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m1" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> time, then fix it ratio:</span> To calculate the time with this method, we take the sampling time for the model, add review time <math alttext="R_{T}" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m2" intent=":literal"><semantics><msub><mi>R</mi><mi>T</mi></msub><annotation encoding="application/x-tex">R_{T}</annotation></semantics></math> for an expert to assess quality, and then with probability (<math alttext="1-w_{i}" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m3" intent=":literal"><semantics><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">1-w_{i}</annotation></semantics></math>) add in the human completion time for any fixes needed for that model for a task <math alttext="i" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m4" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
, to obtain <math alttext="T_{1,i}" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m5" intent=":literal"><semantics><msub><mi>T</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">T_{1,i}</annotation></semantics></math> and analogously <math alttext="C_{1,i}" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m6" intent=":literal"><semantics><msub><mi>C</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">C_{1,i}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx1">
<tbody id="A1.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}[T_{1,i}]" class="ltx_Math" display="inline" id="A1.E1.m1" intent=":literal"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>T</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}[T_{1,i}]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=M_{T,i}+R_{T,i}+(1-w_{i})H_{T,i}" class="ltx_Math" display="inline" id="A1.E1.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><msub><mi>M</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=M_{T,i}+R_{T,i}+(1-w_{i})H_{T,i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="A1.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}[C_{1,i}]" class="ltx_Math" display="inline" id="A1.E2.m1" intent=":literal"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>C</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}[C_{1,i}]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=M_{C,i}+R_{C,i}+(1-w_{i})H_{C,i}" class="ltx_Math" display="inline" id="A1.E2.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><msub><mi>M</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=M_{C,i}+R_{C,i}+(1-w_{i})H_{C,i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.I2.i2.p2">
<p class="ltx_p">The average time spent is <math alttext="T_{1}=\mathbb{E}[T_{1,i}]" class="ltx_Math" display="inline" id="A1.I2.i2.p2.m1" intent=":literal"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>T</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">T_{1}=\mathbb{E}[T_{1,i}]</annotation></semantics></math>, marginalizing over all tasks <math alttext="i" class="ltx_Math" display="inline" id="A1.I2.i2.p2.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, similarly with <math alttext="C_{1}" class="ltx_Math" display="inline" id="A1.I2.i2.p2.m3" intent=":literal"><semantics><msub><mi>C</mi><mn>1</mn></msub><annotation encoding="application/x-tex">C_{1}</annotation></semantics></math>. This proxies the setup where a human tries using GPT-5 for a task, assesses its quality, and then does the task themselves if the deliverable quality is below their quality bar. Our plug-in estimate of the time savings ratio is: <math alttext="H_{T}/(M_{T}+R_{T}+(1-w)H_{T})=H_{T}/\hat{T}_{1}" class="ltx_Math" display="inline" id="A1.I2.i2.p2.m4" intent=":literal"><semantics><mrow><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>M</mi><mi>T</mi></msub><mo>+</mo><msub><mi>R</mi><mi>T</mi></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>w</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mi>T</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>/</mo><msub><mover accent="true"><mi>T</mi><mo>^</mo></mover><mn>1</mn></msub></mrow></mrow><annotation encoding="application/x-tex">H_{T}/(M_{T}+R_{T}+(1-w)H_{T})=H_{T}/\hat{T}_{1}</annotation></semantics></math>, where we use the empirical mean <math alttext="\hat{T}_{1}" class="ltx_Math" display="inline" id="A1.I2.i2.p2.m5" intent=":literal"><semantics><msub><mover accent="true"><mi>T</mi><mo>^</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\hat{T}_{1}</annotation></semantics></math>. The analogous cost ratio is <math alttext="H_{C}/(M_{C}+R_{C}+(1-w)H_{C})" class="ltx_Math" display="inline" id="A1.I2.i2.p2.m6" intent=":literal"><semantics><mrow><msub><mi>H</mi><mi>C</mi></msub><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>M</mi><mi>C</mi></msub><mo>+</mo><msub><mi>R</mi><mi>C</mi></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>w</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mi>C</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H_{C}/(M_{C}+R_{C}+(1-w)H_{C})</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="A1.I2.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Try <math alttext="n" class="ltx_Math" display="inline" id="A1.I2.i3.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> times, then fix it ratio:</span> To calculate the time with this method, we take the sampling time for the model, add review time <math alttext="R_{T}" class="ltx_Math" display="inline" id="A1.I2.i3.p1.m2" intent=":literal"><semantics><msub><mi>R</mi><mi>T</mi></msub><annotation encoding="application/x-tex">R_{T}</annotation></semantics></math> for an expert to assess quality, and then add in the human completion time for any fixes needed for that model (based on <math alttext="1-w_{i}" class="ltx_Math" display="inline" id="A1.I2.i3.p1.m3" intent=":literal"><semantics><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">1-w_{i}</annotation></semantics></math>) <span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>We are over-penalizing the model here, because the win rate after each completion likely goes up (because the professional will adjust the prompt to the model to fix the errors) and the review time also goes down as the professional gets more comfortable with the task.</span></span></span>. We repeat this across <math alttext="n" class="ltx_Math" display="inline" id="A1.I2.i3.p1.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> resamples and re-assess steps before the human steps in to fix it:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.I2.i3.p2">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx2">
<tbody id="A1.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}[T_{n,i}]" class="ltx_Math" display="inline" id="A1.E3.m1" intent=":literal"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>T</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}[T_{n,i}]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{k=1}^{n}\bigl((1-w_{i})^{\,k-1}(M_{T,i}+R_{T,i})\bigr)\;+\;(1-w_{i})^{\,n}H_{T,i}" class="ltx_Math" display="inline" id="A1.E3.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>M</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em" rspace="0.280em">)</mo></mrow></mrow><mo rspace="0.502em">+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{k=1}^{n}\bigl((1-w_{i})^{\,k-1}(M_{T,i}+R_{T,i})\bigr)\;+\;(1-w_{i})^{\,n}H_{T,i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="A1.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\left(M_{T,i}+R_{T,i}\right)\frac{1-(1-w_{i})^{n}}{w_{i}}+(1-w_{i})^{n}H_{T,i}" class="ltx_Math" display="inline" id="A1.E4.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><msub><mi>M</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><mrow><mn>1</mn><mo>−</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><msub><mi>w</mi><mi>i</mi></msub></mfrac></mstyle></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mrow><mi>T</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\left(M_{T,i}+R_{T,i}\right)\frac{1-(1-w_{i})^{n}}{w_{i}}+(1-w_{i})^{n}H_{T,i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="A1.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}[C_{n,i}]" class="ltx_Math" display="inline" id="A1.E5.m1" intent=":literal"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>C</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}[C_{n,i}]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{k=1}^{n}\bigl((1-w_{i})^{\,k-1}(M_{C,i}+R_{C,i})\bigr)\;+\;(1-w_{i})^{\,n}H_{C,i}" class="ltx_Math" display="inline" id="A1.E5.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>M</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em" rspace="0.280em">)</mo></mrow></mrow><mo rspace="0.502em">+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{k=1}^{n}\bigl((1-w_{i})^{\,k-1}(M_{C,i}+R_{C,i})\bigr)\;+\;(1-w_{i})^{\,n}H_{C,i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="A1.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\left(M_{C,i}+R_{C,i}\right)\frac{1-(1-w_{i})^{n}}{w_{i}}+(1-w_{i})^{n}H_{C,i}" class="ltx_Math" display="inline" id="A1.E6.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><msub><mi>M</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><mrow><mn>1</mn><mo>−</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><msub><mi>w</mi><mi>i</mi></msub></mfrac></mstyle></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mrow><mi>C</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\left(M_{C,i}+R_{C,i}\right)\frac{1-(1-w_{i})^{n}}{w_{i}}+(1-w_{i})^{n}H_{C,i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="A1.I2.i3.p3">
<p class="ltx_p">This proxies the setup where a human tries <math alttext="n" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> rounds of using GPT-5 for a task, then assesses its quality each time, and then does the task themselves if the model quality is below their quality bar after all attempts. As before, the average time spent is <math alttext="T_{n}=\mathbb{E}[T_{n,i}]" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m2" intent=":literal"><semantics><mrow><msub><mi>T</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>T</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">T_{n}=\mathbb{E}[T_{n,i}]</annotation></semantics></math>, marginalizing over all tasks <math alttext="i" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, similarly with <math alttext="C_{n}" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m4" intent=":literal"><semantics><msub><mi>C</mi><mi>n</mi></msub><annotation encoding="application/x-tex">C_{n}</annotation></semantics></math>. Therefore, as <math alttext="n\to\infty" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m5" intent=":literal"><semantics><mrow><mi>n</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">n\to\infty</annotation></semantics></math>, with <math alttext="w&gt;0" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m6" intent=":literal"><semantics><mrow><mi>w</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w&gt;0</annotation></semantics></math>, the time savings are <math alttext="H_{T}/((M_{T}+R_{T})/w)" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m7" intent=":literal"><semantics><mrow><msub><mi>H</mi><mi>T</mi></msub><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>M</mi><mi>T</mi></msub><mo>+</mo><msub><mi>R</mi><mi>T</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mi>w</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H_{T}/((M_{T}+R_{T})/w)</annotation></semantics></math> times faster and cost savings are <math alttext="H_{C}/((M_{C}+R_{C})/w)" class="ltx_Math" display="inline" id="A1.I2.i3.p3.m8" intent=":literal"><semantics><mrow><msub><mi>H</mi><mi>C</mi></msub><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>M</mi><mi>C</mi></msub><mo>+</mo><msub><mi>R</mi><mi>C</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mi>w</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H_{C}/((M_{C}+R_{C})/w)</annotation></semantics></math> times cheaper than human experts.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="A1.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Speed and cost improvements under different review strategies.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding:0.6pt 4.0pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" style="padding:0.6pt 4.0pt;">Speed improvement</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" style="padding:0.6pt 4.0pt;">Cost improvement</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding:0.6pt 4.0pt;">Model</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding:0.6pt 4.0pt;">Win rate</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding:0.6pt 4.0pt;">Naive</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding:0.6pt 4.0pt;">Try 1x</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding:0.6pt 4.0pt;">Try <math alttext="n" class="ltx_Math" display="inline" id="A1.T2.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>x</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding:0.6pt 4.0pt;">Naive</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding:0.6pt 4.0pt;">Try 1x</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding:0.6pt 4.0pt;">Try <math alttext="n" class="ltx_Math" display="inline" id="A1.T2.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>x</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;">gpt-4o</th>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">12.5%</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">327x</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.87x</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.46x</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">5172x</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.90x</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.53x</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">o4-mini</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">29.1%</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">186x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.02x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.06x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1265x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.06x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.22x</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">o3</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">35.2%</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">161x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.08x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.28x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">480x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.13x</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.47x</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.6pt 4.0pt;">gpt-5</th>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">39.0%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">90x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.12x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.39x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">474x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.18x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.63x</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS1.p3">
<p class="ltx_p">When incorporating time to review and redo work, the payoff from using a model shrinks. We do not include consideration of the time taken to review a human professional deliverable, although this would commonly occur for tasks in GDPval (either self-review of the professional’s own work or review by a supervisor of a team member’s work). We also do not include the possibility that the human deliverable is also undesirable. One further limitation of this analysis is that it does not capture the cost of catastrophic mistakes, which can be disproportionately expensive in some domains.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Win rates by sector</h4>
<div class="ltx_para" id="A1.SS2.SSS2.p1">
<p class="ltx_p">We provide a sector-level breakdown of win rates in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.F10" title="In A.2.2 Win rates by sector ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a>. Results differ across industries. Some sectors have low win rates for all models, while in others (e.g., Government, Retail Trade, and Wholesale Trade), the strongest models approach parity on GDPval tasks.</p>
</div>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="371" id="A1.F10.g1" src="assets/gdpval_pairwise_expert_preferences_by_sector_3x3.png" width="494"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Win rate by sector</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.3 </span>Win rates by occupation</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS3.p1">
<p class="ltx_p">We provide a detailed breakdown of win rates by occupation in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.F11" title="In A.2.3 Win rates by occupation ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a>. Results vary: some occupations show consistently low win rates across all models, while others display near parity among multiple models.</p>
</div>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="692" id="A1.F11.g1" src="assets/gdpval_pairwise_expert_preferences_by_occupation_grid.png" width="548"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Win rate by occupation</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.4 </span>Win rates by deliverable</h4>
<div class="ltx_para" id="A1.SS2.SSS4.p1">
<p class="ltx_p">We report win rates by deliverable type in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.F12" title="In A.2.4 Win rates by deliverable ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">12</span></a>. Performance varies across formats, with Claude achieving the best results for all deliverables except pure text. GPT-5 high leads for pure text outputs, though overall win rates remain low.</p>
</div>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="219" id="A1.F12.g1" src="assets/model_winrate_by_deliverable_file_extension.png" width="439"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Win rate by deliverable file type</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.5 </span>Win rates by time to complete</h4>
<div class="ltx_para" id="A1.SS2.SSS5.p1">
<p class="ltx_p">We report win rates by task duration in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#A1.F13" title="In A.2.5 Win rates by time to complete ‣ A.2 Additional Detail on Experimental Results ‣ Appendix A Appendix ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">13</span></a>. Win rates are highest for shorter tasks (0–2 hours) and decline steadily as completion time increases. This indicates that models perform best on faster, less time-intensive tasks.</p>
</div>
<figure class="ltx_figure" id="A1.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="163" id="A1.F13.g1" src="assets/gdpval_model_winrate_by_time_to_complete.png" width="494"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Win rate by time to complete task</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.6 </span>Additional detail on model failures analysis</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS6.p1">
<p class="ltx_p">We took the subset of GPT-5 model failures (tasks where the GPT-5 deliverable lost to the human expert), and then we asked other expert occupational graders to rate these subset samples as:</p>
<ol class="ltx_enumerate" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Catastrophic:</span> The model completion would be catastrophic if used in real life because it is harmful or dangerously wrong (e.g., insulting a customer, giving the wrong diagnosis, recommending fraud, or suggesting actions that will cause physical harm).</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Bad:</span> The completion was bad and not fit for use, but not offensive or dangerous (e.g., rambling nonsense, completely irrelevant, or incoherent answers).</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I3.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Acceptable but subpar:</span> The completion was acceptable (and could be used) but the human produced a stronger response (e.g., model response lacked helpful detail compared to the human).</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="A1.I3.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">N/A:</span> Disagree with original expert grader; the model completion was better than the human completion.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="A1.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="247" id="A1.F14.g1" src="assets/failures_analysis.png" width="329"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Experts rated GPT-5 model failures by categorized by severity of failure.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS6.p2">
<p class="ltx_p">The most common categorization of a GPT-5 model failure was “acceptable but subpar.” Another roughly 29% of ratings were for bad or catastrophic (with roughly 3% of failures marked as catastrophic). The 23% of ratings for “model better” roughly corresponds to the level of inter-rater agreement we observed in <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#S2.F4.sf2" title="In Figure 4 ‣ 2.4 Task Quality Control Pipeline ‣ 2 Task Creation ‣ GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">4(b)</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.7 </span>Under-contextualized GDPval</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS7.p1">
<p class="ltx_p">To assess how models handle task ambiguity, we created a modified version of GDPval with deliberately lower-context prompts. These shorter prompts omitted additional context such as where to locate specific data within reference files, how to approach the problem, or detailed formatting expectations for the final deliverable; the models had to “figure it out.” On average, these revised prompts were 42% the length (by token count) of the original prompts.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS7.p2">
<p class="ltx_p">This setting helped measure an aspect of professional knowledge work previously unaddressed in our evaluation: navigating ambiguity by figuring out what to work on and where to get the necessary inputs. We collected and graded GPT-5 completions with expert human graders and found the model’s performance was worse on under-specified prompts. In particular, the models struggled to figure out context.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS7.p3">
<p class="ltx_p">As a note: this experiment was run on an earlier version of the GDPval gold subset, and therefore the observed win rates do not match those in the main text of the paper.</p>
</div>
<figure class="ltx_figure" id="A1.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="A1.F15.g1" src="assets/underspecified_gdpval.png" width="329"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>On the underspecified version of GDPval, GPT-5 performed worse as it struggled to figure out requisite context.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Additional Detail on Prompt-Tuning</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p">Here is the prompt we give the agent to elicit capabilities (lightly edited to remove some specific details of our scaffolding setup).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="1025.71" id="A1.SS3.p2.pic1" overflow="visible" version="1.1" viewbox="0 0 600 1025.71" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,1025.71) matrix(1 0 0 -1 0 0)"><clippath id="pgfcp13"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 9.84 L 0 1015.87 C 0 1021.3 4.41 1025.71 9.84 1025.71 L 590.16 1025.71 C 595.59 1025.71 600 1021.3 600 1015.87 L 600 9.84 C 600 4.41 595.59 0 590.16 0 L 9.84 0 C 4.41 0 0 4.41 0 9.84 Z"></path></clippath><g fill-rule="evenodd"><g fill="#000000" fill-opacity="1.0" style="--ltx-fill-color:#000000;"><path d="M 0 9.84 L 0 1015.87 C 0 1021.3 4.41 1025.71 9.84 1025.71 L 590.16 1025.71 C 595.59 1025.71 600 1021.3 600 1015.87 L 600 9.84 C 600 4.41 595.59 0 590.16 0 L 9.84 0 C 4.41 0 0 4.41 0 9.84 Z M 1.97 9.84 L 1.97 1001.71 L 598.03 1001.71 L 598.03 9.84 C 598.03 5.49 594.51 1.97 590.16 1.97 L 9.84 1.97 C 5.49 1.97 1.97 5.49 1.97 9.84 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 9.84 L 1.97 1001.71 L 598.03 1001.71 L 598.03 9.84 C 598.03 5.49 594.51 1.97 590.16 1.97 L 9.84 1.97 C 5.49 1.97 1.97 5.49 1.97 9.84 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 1010.31)"><foreignobject color="#FFFFFF" height="12.18" overflow="visible" style="--ltx-fg-color:#FFFFFF;--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.49)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="976.12" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:70.35em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 973.43)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p">Special characters
- Never use the character ‑ (U+2011), since it will render poorly on some people’s computers. Instead, always use - (U+002D) instead.
- Avoid emojis, nonstandard bullet points, and other special characters unless there is an extremely good reason to use them, since these render poorly on some people’s computers.</span>
<span class="ltx_p">Graphics embedded within PDFs/slides
- Make sure that any diagrams or plots are large enough to be legible (though not so large that they are ugly or cut off). In most cases they should be at least half the page width.
- Plots and charts to visualize data are good. Simple graphics (like a flowchart with arrows) are good. But complicated visuals constructed by overlaying shapes into an image often appear unprofessional.</span>
<span class="ltx_p">PDFs
- Always use LibreOffice to create the PDF (it must be LibreOffice! If LibreOffice is not installed, you can install it yourself). Other libraries sometimes show weird artifacts on some computers.</span>
<span class="ltx_p">Fonts
- Always use fonts which are available across all platforms. We recommend Noto Sans / Noto Serif unless there is an extremely good reason to use something else. If you must use another font, embed the font in the pptx/word/etc doc.</span>
<span class="ltx_p">Deliverable text
- Do not link to submitted files in the deliverable text (links are not supported on the interface where these will be viewed).
- Ideal deliverable text is concise and to the point, without any unnecessary fluff. 4 sentences max.
- Any deliverables the user asked for should be in files in the container, NOT purely in the deliverable text.
- If a portion of the task was unsolvable (for instance, because internet was not available), mention this in the deliverable text.
- Your submission should be complete and self-contained. Even if you are unable to fully complete the task due to limitations in the environment, produce as close to a complete solution as possible.</span>
<span class="ltx_p">Verbosity
Always be clear and comprehensive, but avoid extra verbosity when possible.</span>
<span class="ltx_p">Filetypes
If the prompt does not request a specific filetype, use ”standard” filetypes like PDF, PPTX, DOCX, XLSX, MP4, ZIP, etc.</span>
<span class="ltx_p">Video files (mp4, mov)
Extract a string of images from the video files and check the images to see whether the visual elements are corrupted.</span>
<span class="ltx_p">Mandatory formatting checks
Before you submit your deliverable, you MUST perform the following mandatory formatting checks. Take your time, do these thoroughly, they are extremely important!</span>
<span class="ltx_p">STEP 1: Convert all visual deliverables to PNGs using LibreOffice. This includes pptx, docx, pdf, xlsx, etc. Convert it so that each page or slide is a separate PNG. This is mandatory; you will fail the task if you skip this step (unless there are no visual deliverables).
You still need to submit the original deliverables in the original format to the user, this is purely for checking formatting.</span>
<span class="ltx_p">STEP 2: Display the PNGs. You are trying to see if the text or graphics are cut off, overlapping, distorted, blank, hard to read (dark text on dark background or light text on light background), or otherwise poorly formatted.
Look at each image thoroughly, zoom in if you need to see more closely.
Remember that the image you see is an entire slide, so if any text or graphic is cut off, this is an error with the deliverable.</span>
<span class="ltx_p">STEP 3: Programmatic formatting checks. For highly visual submissions (e.g. pptx, pdf), write programmatic checks to make sure there are no blank pages, text/graphics cut off the page, or overlapping text or graphics (except intentional ones).
Also check that if there is a page or slide limit, it is respected.</span>
<span class="ltx_p">STEP 4: Summarize the prompt’s deliverable instructions, and match that to the portion of the deliverable that addresses it.</span>
<span class="ltx_p">STEP 5: Right before submitting, check that the deliverables you have produced are exactly what you want to submit: deliverables should contain exactly the files you want to submit, with no extra files.
Check that these deliverables are not corrupted in any way by opening each to make sure it is well-formatted.</span>
<span class="ltx_p">If any of these checks reveal a formatting issue, fix them and go through steps 1-5 again. Take your time, be thorough, remember you can zoom in on details.</span>
<span class="ltx_p">This is IMPORTANT and MANDATORY, go through each step one-by-one meticulously! Every formatting error is a MAJOR ISSUE THAT YOU NEED TO FIX! There is no time limit, be thorough, go slide by slide or page by page.</span>
<span class="ltx_p">Finally – on the last line of your output text, add CONFIDENCE[XX], where XX is an integer between 0 and 100, inclusive, indicating your confidence that the submission is correct, follows instructions, and is well-formatted.</span>
</span></span></span></foreignobject></g></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p3">
<p class="ltx_p">We performed best-of-N sampling by prompting a GPT-5 grader with the prompt, reference files, and deliverable files for four different submissions, then asking it to pick the best.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Additional Task Characteristics</h3>
<figure class="ltx_table" id="A1.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Summary statistics for GDPval gold subset tasks</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding:0.6pt 4.0pt;"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Mean</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Std</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Min</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">25%</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">50%</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">75%</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Max</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;">Overall quality (1–5)</th>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">4.47</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.32</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">3.18</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">4.30</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">4.50</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">4.70</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">5.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">Difficulty (1–5)</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">3.32</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">0.95</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">3.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">3.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">4.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">Representativeness (1–5)</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">4.50</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">0.74</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">2.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">4.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">Avg time to complete (hrs)</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">9.49</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">13.75</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">0.50</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">2.38</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">10.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">100.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.6pt 4.0pt;">Dollar value of task</th>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$398.46</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$599.45</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$12.59</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$93.72</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$174.81</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$386.03</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$4,114.20</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Summary statistics for GDPval full set tasks</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding:0.6pt 4.0pt;"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Mean</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Std</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Min</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">25%</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">50%</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">75%</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Max</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;">Overall quality (1–5)</th>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">4.55</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.43</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">2.00</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">4.33</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">4.56</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">5.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">5.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">Difficulty (1–5)</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">3.20</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">0.92</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">3.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">3.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">4.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">Representativeness (1–5)</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">4.43</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">0.76</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">1.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">4.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">5.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">Avg time to complete (hrs)</th>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">8.63</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">24.70</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">0.25</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">2.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">4.00</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">8.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">605.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.6pt 4.0pt;">Dollar value of task</th>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$391.44</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$1,296.67</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$8.53</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$70.70</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$147.31</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$354.12</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">$32,028.70</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="A1.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.1 </span>Files and attachments</h4>
<div class="ltx_para" id="A1.SS4.SSS1.p1">
<p class="ltx_p">Many traditional evaluations rely on text-in/text-out task formats. GDPval tasks incorporate a broad range of real-world file types (such as spreadsheets, documents, presentations, images, audio, video, and specialized formats like CAD). 67.7% of tasks required interaction with at least one reference file.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>File counts for GDPval gold set tasks</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding:0.6pt 4.0pt;"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Mean</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Std</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Min</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">25%</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">50%</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">75%</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Max</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;">Reference files</th>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">1.92</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">3.47</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.00</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">0.00</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">1.00</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">2.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">38.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.6pt 4.0pt;">Deliverable files</th>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.54</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">2.64</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">0.00</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.00</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.00</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">36.00</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.2 </span>O*NET Tasks, Skills, and Work Activities</h4>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS2.p1">
<p class="ltx_p">To ensure broad occupational representativeness, we analyzed the O*NET <a class="ltx_ref ltx_href" href="https://www.onetcenter.org/dictionary/28.3/excel/task_statements.html" title="">tasks</a>, <a class="ltx_ref ltx_href" href="https://www.onetcenter.org/dictionary/28.3/excel/skills.html" title="">skills</a>, and general <a class="ltx_ref ltx_href" href="https://www.onetcenter.org/dictionary/28.3/excel/work_activities.html" title="">work activities</a> represented by GDPval tasks. The dataset covered 208 unique O*NET tasks, 25 occupational skills, and 26 work activities.</p>
</div>
<div class="ltx_para" id="A1.SS4.SSS2.p2">
<p class="ltx_p">Most GDPval tasks involve multiple O*NET tasks, skills, and work activities.</p>
</div>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>O*NET Tasks, Skills, and Work Activities coverage in gold set</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt" style="padding:0.6pt 4.0pt;"></td>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Total unique in O*NET</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Total in gold subset</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Coverage (%)</span></th>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding:0.6pt 4.0pt;">O*NET Skills</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">35</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">25</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">71.4%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding:0.6pt 4.0pt;">O*NET Work Activities</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">41</td>
<td class="ltx_td ltx_align_right" style="padding:0.6pt 4.0pt;">26</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">63.4%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.6pt 4.0pt;">O*NET Tasks</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">1,470</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">208</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">14.15%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.3 </span>Task specification</h4>
<div class="ltx_para" id="A1.SS4.SSS3.p1">
<p class="ltx_p">Occupational experts conducting human grading rated the specificity of instructions provided in each prompt. <span class="ltx_text ltx_font_bold">89.07%</span> of tasks were rated as well-specified, indicating the instructions closely matched real-world expectations of clarity and detail.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Task specification scores</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">Label</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">%, gold set</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:0.6pt 4.0pt;"><span class="ltx_text ltx_font_bold">%, full set</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.6pt 4.0pt;">Underspecified</th>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">8.28%</td>
<td class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding:0.6pt 4.0pt;">8.41%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.6pt 4.0pt;">Well-specified</th>
<td class="ltx_td ltx_nopad_r ltx_align_right" style="padding:0.6pt 4.0pt;">89.07%</td>
<td class="ltx_td ltx_nopad_l ltx_align_right" style="padding:0.6pt 4.0pt;">89.34%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.6pt 4.0pt;">Overspecified</th>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">2.66%</td>
<td class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding:0.6pt 4.0pt;">2.26%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.4.4 </span>Task Representativeness</h4>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS4.p1">
<dl class="ltx_description" id="A1.I4">
<dt class="ltx_item" id="A1.I4.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Professional Services</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> Technology and intellectual property attorney with partner roles at multiple AmLaw 100 firms in New York and California, and 15+ years of experience advising clients on emerging technologies, advertising, antitrust, and cross-border disputes and transactions. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> Legal tasks included details that felt true to practice, like ambiguous fact patterns, disclosure of relevant legal considerations along with non-legal business goals, and realistic reference documents.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Healthcare</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> Nursing professional with 18+ years of expertise in emergency medicine, renal management, care coordination, and healthcare operations. Skilled in quality assurance, case management, and professional education. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> These tasks captured the complexity of the role, requiring not only a keen ear for the physician’s words, but also careful attention to clinical accuracy and professional formatting.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Retail Trade</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> Strategic retail executive with 15 years of experience growing prestige and niche beauty brands through national account leadership, $1B+ P&amp;L ownership, and data-driven omnichannel strategies.
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> These tasks mirrored the work I performed regularly, including developing revenue forecasts, conducting competitive analysis, building executive-level presentations, and driving strategic initiatives for key retail partners within a global organization.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix4"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Finance</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> Fintech and Wall Street leader with 20+ years of experience in wealth management, asset management, and capital markets across global institutions and startups. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> They reflected real-world scenarios that were nuanced and individualized, situations that only someone with years of experience in the field would fully comprehend. The language and details used in the tasks were directly drawn from actual industry practice, making them authentic and grounded in real-world application.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix5"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Wholesale Trade</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> National Accounts Sales Manager for US, China, and Sweden based brands/factories with over 25 years of experience selling to US based retailers. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> All the tasks were in fact based upon real world tasks with back-up reference files and real-world data.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix6"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Manufacturing</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> Lead Industrial Engineer with 5+ years of experience managing large-scale projects and leading teams of 10+ engineers in industrial operations. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> The redesign tasks stood out as especially true to real-world practice because they included specific design components and blocks, along with detailed drawings that incorporated precise measurements. They emphasized practical considerations such as visibility and optimizing walking distances to improve overall productivity, exactly the kind of detail-oriented focus that reflects actual engineering and operational priorities.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix7"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Government</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> Executive leader with 15+ years working at strategic and operational levels in government and non-profit sectors in housing, human service and labor market programs. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> Many of the tasks demand the integration of multiple sources of information, nuanced decision-making, and tailored the work to varied audiences we serve in the workplace.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix8"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Real Estate and Leasing</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> Seasoned commercial real estate broker with 10 years of experience in investment sales, leasing, and managing real estate offices and agents. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> The tasks capture the dynamics and expertise unique to specific sectors and settings.</p>
</div>
</dd>
<dt class="ltx_item" id="A1.I4.ix9"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">Information</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I4.ix9.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Qualification:</span> An experienced senior journalist and content leader with over 20 years in top-tier media, global corporations, and high-growth startups. 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic">Quote:</span> Most importantly, the tasks are anchored in real-world challenges and workplace goals. They push past obstacles, achieve workplace goals, and deliver real-world solutions and products.</p>
</div>
</dd>
</dl>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS4.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Additional Detail about Expert Qualifications</span>
Less than 10% of applicants were selected to contribute tasks to our full set. The industry experts also brought occupational diversity, representing different company sizes, locations, and sub-specialties. Each occupation had a minimum of 5 qualified professionals.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS4.p3">
<p class="ltx_p">Experts for each occupation were required to have previous experience in that specific occupation and sector based on the O*NET occupation definitions <cite class="ltx_cite ltx_citemacro_citep">(U.S. Bureau of Labor
Statistics, <a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib22" title="">2025a</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Further Detail on Task Quality Control</h3>
<section class="ltx_subsubsection" id="A1.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.1 </span>Model-in-the-loop Task Review</h4>
<div class="ltx_para ltx_noindent" id="A1.SS5.SSS1.p1">
<p class="ltx_p">We used OpenAI models to automatically screen each task submission across a variety of criteria and flag possible errors or omissions including: ensuring the task is relevant to the selected O*NET occupation, verifying the request involved tasks performed primarily on a computer, flagging if the task complexity was too simple (e.g., if the task seemed like 5 minutes of work instead of a longer-term piece of work), and indicating if there were no deliverable and reference files attached.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS5.SSS1.p2">
<p class="ltx_p">Because models can make mistakes, experts were instructed to take model feedback as a suggestion rather than a direction. Experts retained final responsibility for task accuracy and completeness; the model did not autonomously alter tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.2 </span>Human Expert Reviewers</h4>
<div class="ltx_para ltx_noindent" id="A1.SS5.SSS2.p1">
<p class="ltx_p">Human reviewers conducted multiple rounds of review on each task. Reviewers were primarily sourced from the original expert pool based on demonstrated excellence in task creation. Initially, our researchers manually reviewed all tasks to identify experts who produced consistently high-quality tasks; these individuals were trained and promoted to reviewers. The most skilled reviewers were further trained to become lead reviewers, responsible for identifying, mentoring, and promoting additional qualified reviewers from within the expert pool. Throughout the review process, the research team regularly performed quality-control checks on tasks signed off by reviewers, ensuring ongoing alignment and quality standards.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.3 </span>Iterative review process</h4>
<div class="ltx_para ltx_noindent" id="A1.SS5.SSS3.p1">
<p class="ltx_p">The iterative review process included at least the following 3 stages:</p>
<ol class="ltx_enumerate" id="A1.I5">
<li class="ltx_item" id="A1.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I5.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Generalist initial review</span>: A generalist reviewer confirmed the task adhered to project requirements.</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I5.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Occupation-specific expert review</span>: An occupation-specific reviewer assessed the representativeness of the task for the occupation, and confirmed that the task was possible for another member of the occupation to complete with the provided context.</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="A1.I5.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Final iterative reviewer feedback loop</span>: A third expert reviewer provided iterative feedback and worked with experts until the task met our rigorous quality standards.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Automated Grader Details</h3>
<section class="ltx_subsubsection" id="A1.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.1 </span>Automated Grader Consensus Metrics</h4>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS1.p1">
<p class="ltx_p">To measure automated grader performance, we measured the agreement rate between scores given by the automated grader vs. human expert graders for the same sample. We also compared grading agreement between human experts who had graded the same sample.</p>
</div>
<section class="ltx_paragraph" id="A1.SS6.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Human-automated grader Agreement.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS1.Px1.p1">
<p class="ltx_p">For a given sample <math alttext="s" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>, let the human score <math alttext="H" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p1.m2" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> and automated grader score <math alttext="A" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p1.m3" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> take values in <math alttext="\{0,0.5,1\}" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p1.m4" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0,0.5,1\}</annotation></semantics></math>, where <math alttext="1" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p1.m5" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> indicates preference for the model deliverable, <math alttext="0" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p1.m6" intent=":literal"><mn>0</mn></math> indicates preference for the human deliverable, and <math alttext="0.5" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p1.m7" intent=":literal"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math> indicates a tie.
The agreement between human and automated grader is defined as</p>
<table class="ltx_equation ltx_eqn_table" id="A1.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A_{s}^{\mathrm{HA}}=\mathbb{E}\bigl[1-|H-A|\bigr]." class="ltx_Math" display="block" id="A1.Ex1.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>A</mi><mi>s</mi><mi>HA</mi></msubsup><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">[</mo><mrow><mn>1</mn><mo>−</mo><mrow><mo stretchy="false">|</mo><mrow><mi>H</mi><mo>−</mo><mi>A</mi></mrow><mo stretchy="false">|</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">A_{s}^{\mathrm{HA}}=\mathbb{E}\bigl[1-|H-A|\bigr].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS1.Px1.p2">
<p class="ltx_p">The model-level human–automated grader agreement is the mean of <math alttext="A_{s}^{\mathrm{HA}}" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px1.p2.m1" intent=":literal"><semantics><msubsup><mi>A</mi><mi>s</mi><mi>HA</mi></msubsup><annotation encoding="application/x-tex">A_{s}^{\mathrm{HA}}</annotation></semantics></math> over all samples for that model.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS6.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Human Inter-Rater Agreement.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS1.Px2.p1">
<p class="ltx_p">For a given sample <math alttext="s" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px2.p1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>, let the human scores <math alttext="H_{1}" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px2.p1.m2" intent=":literal"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_{1}</annotation></semantics></math> and <math alttext="H_{2}" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px2.p1.m3" intent=":literal"><semantics><msub><mi>H</mi><mn>2</mn></msub><annotation encoding="application/x-tex">H_{2}</annotation></semantics></math> take values in <math alttext="p\in\{0,0.5,1\}" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px2.p1.m4" intent=":literal"><semantics><mrow><mi>p</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">p\in\{0,0.5,1\}</annotation></semantics></math>. We measure human inter-rater agreement as the following expectation over two randomly sampled human ratings</p>
<table class="ltx_equation ltx_eqn_table" id="A1.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A_{s}^{\mathrm{HH}}=\mathbb{E}\bigl[1-|H_{1}-H_{2}|\bigr]." class="ltx_Math" display="block" id="A1.Ex2.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>A</mi><mi>s</mi><mi>HH</mi></msubsup><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">[</mo><mrow><mn>1</mn><mo>−</mo><mrow><mo stretchy="false">|</mo><mrow><msub><mi>H</mi><mn>1</mn></msub><mo>−</mo><msub><mi>H</mi><mn>2</mn></msub></mrow><mo stretchy="false">|</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">A_{s}^{\mathrm{HH}}=\mathbb{E}\bigl[1-|H_{1}-H_{2}|\bigr].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS1.Px2.p2">
<p class="ltx_p">For a given sample, we estimate this quantity by the empirical mean over all pairs of ratings for that sample. The final human inter-rater agreement for a model is the mean of these sample-level scores over all samples with at least two human graders. Existing grader inter-reliability statistics such as Cohen’s kappa, Fleiss’ kappa, and Krippendorff’s alpha are less directly applicable here, since our graders output ordinal scores in <math alttext="\{0,0.5,1\}" class="ltx_Math" display="inline" id="A1.SS6.SSS1.Px2.p2.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0,0.5,1\}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="A1.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.2 </span>Automated Grader Correlation Results</h4>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS2.p1">
<p class="ltx_p">Over three automated grader sweeps on our dataset<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Metrics were calculated over all samples where the automated grader did not encounter systems errors and returned a valid score. We also excluded 12 tasks (out of the 220 in our open-sourced eval set) that the automated grader frequently could not grade or was less likely to grade reliably due to its limitations, described later.</span></span></span>, average human-automated grader agreement was 65.7% and human inter-rater agreement was 70.8%. Plots below show 95% confidence intervals obtained by bootstrapping (resampling with replacement the available automated grader scores or human grades for each sample, computing the mean per sample, and averaging across all samples or for the specified model).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS2.p2">
<p class="ltx_p">Our automated grader, based on GPT-5-high, shows lower correlation with human expert graders when assessing outputs from capable OpenAI models. This aligns with empirical evidence that models often favor their own responses <cite class="ltx_cite ltx_citemacro_cite">Panickssery et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib17" title="">2024</a>)</cite>. Both agreement metrics are highest for less capable models, since their outputs are easier to distinguish from human deliverables and are less likely to be preferred.</p>
</div>
<figure class="ltx_figure" id="A1.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="173" id="A1.F16.g1" src="assets/agreement_with_humans_by_model.png" width="494"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Average human-automated grader agreement is most closely aligned with human inter-rater agreement for non-OpenAI models. Both agreement metrics are highest for less capable models, as they can be more frequently distinguished from human deliverables and are less likely to be chosen.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.3 </span>Automated Grader Limitations</h4>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS3.p1">
<p class="ltx_p">In the open-source set we mark 12 out of 220 tasks as ungradable due to limitations of the automated grader.</p>
<ol class="ltx_enumerate" id="A1.I6">
<li class="ltx_item" id="A1.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I6.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Internet Access:</span> Tasks which strictly require internet (e.g., tasks that ask agents to find music online and download it) are not possible to grade because the grader does not have internet access.</p>
</div>
</li>
<li class="ltx_item" id="A1.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I6.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Python</span>: The automated grader operates in a container that only allows for running Python. Because of this, we excluded 3 Software Developers tasks that require running other languages and downloading external dependencies to properly test.</p>
</div>
</li>
<li class="ltx_item" id="A1.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I6.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Font Packages</span> Although the automated grader has most metrically-identical fonts (e.g., Liberation Sans instead of Arial), some font packages used in human deliverables still causes certain deliverables to be rendered differently than they would appear on a computer that has these fonts installed.</p>
</div>
</li>
<li class="ltx_item" id="A1.I6.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="A1.I6.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Speech-to-text transcription:</span> The automated grader has limited speech to text functionality inside the container, and struggles with non-voice sounds.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS6.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.6.4 </span>automated grader Packages</h4>
<div class="ltx_para ltx_noindent" id="A1.SS6.SSS4.p1">
<p class="ltx_p">To ensure the model can process a wide variety of file types in <span class="ltx_text ltx_font_typewriter">GDPval</span>, the following packages are pre-installed in the base production Docker image. These were also made available to the agent during sampling of OpenAI models.</p>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<pre class="ltx_verbatim ltx_font_typewriter">
jupyter-client==8.6.1
jupyter-core==5.5.1
jupyter-server==2.14.0
jupyterlab==4.1.8
jupyterlab-pygments==0.3.0
jupyterlab-server==2.27.1
aiohttp==3.9.5
hypercorn==0.14.3
notebook==6.5.1
nbclassic==0.4.5
pydantic==1.10.2
fastapi[all]==0.95.2
websockets==10.3
tqdm==4.64.0
matplotlib==3.6.3
matplotlib-venn==0.11.6
numpy==1.24.0
numpy-financial==1.0.0
scipy==1.14.1
pandas==1.5.3
statsmodels==0.13.5
sympy==1.13.1
seaborn==0.11.2
scikit-learn==1.1.3
nltk==3.9.1
plotnine==0.10.1
shapely==1.7.1
fiona==1.9.2
geopandas==0.10.2
ffmpeg-python==0.2.0
pydub==0.25.1
moviepy==1.0.3
opencv-python==4.5.5.62
Pillow==9.1.0
python-docx==0.8.11
python-pptx==0.6.21
openpyxl==3.0.10
xml-python==0.4.3
geopy==2.2.0
scikit-image==0.20.0
folium==0.12.1
wordcloud==1.9.2
faker==8.13.2
fpdf2==2.8.3
soundfile==0.10.2
kerykeion==2.1.16
pdfkit==0.6.1
pycountry==20.7.3
countryinfo==0.1.2
tabulate==0.9.0
shap==0.39.0
pylog==1.1
pyprover==0.5.6
pytesseract==0.3.8
qrcode==7.3
basemap==1.3.9
pygraphviz==1.7
networkx==2.8.8
pyttsx3==2.90
nashpy==0.0.35
docx2txt==0.8
typing-extensions==4.10.0
torch==2.5.1
torchaudio==2.5.1
torchtext==0.18.0
torchvision==0.20.1
PyMuPDF==1.21.1
pdf2image==1.16.3
pyth3==0.7
h5py==3.8.0
tables==3.8.0
rarfile==4.0
odfpy==1.4.1
pymc==4.0.1
jax==0.2.28
pyxlsb==1.0.8
keras==2.6.0
xgboost==1.4.2
loguru==0.5.3
plotly==5.3.0
graphviz==0.17
fuzzywuzzy==0.18.0
pydot==1.4.2
gensim==4.3.1
pypandoc==1.6.3
einops==0.3.2
reportlab==3.6.12
gradio==2.2.15
mutagen==1.45.1
librosa==0.8.1
svglib==1.1.0
gtts==2.2.3
textblob==0.15.3
rasterio==1.3.3
rdflib==6.0.0
rdkit==2024.9.6
biopython==1.84
cairosvg==2.5.2
markdownify==0.9.3
anytree==2.8.0
pdfplumber==0.6.2
trimesh==3.9.29
svgwrite==1.4.1
pdfrw==0.4
pyzbar==0.1.8
dlib==19.24.2
mtcnn==0.1.1
imgkit==1.2.2
chardet==3.0.4
bokeh==2.4.0
tabula==1.0.5
camelot-py==0.10.1
exchange_calendars==3.4
weasyprint==53.3
pronouncing==0.2.0
cryptography==3.4.8
spacy==3.4.4
requests==2.31.0
mne==0.23.4
pyopenssl==21.0.0
snowflake-connector-python==2.7.12
databricks-sql-connector==0.9.1
ddtrace~=2.8.1
datadog~=0.49.1
pytest~=8.2.0
pytest-cov~=5.0.0
pytest-json-report~=1.5.0
coverage~=7.5.1
pytest-asyncio~=0.23.6
catboost~=1.2.7
lightgbm~=4.5.0
imblearn~=0.0
imbalanced-learn~=0.12.3
rapidfuzz~=3.10.1
</pre>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<p class="ltx_p">We also installed the following additional packages, and we tell the model in the prompt it has access to these additional packages:</p>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<pre class="ltx_verbatim ltx_font_typewriter">
libreoffice
aspose-words==25.8.0
av==11.0.0
cadquery==2.4.0
cadquery-ocp==7.7.0
pedalboard==0.9.9
pyloudnorm==0.1.1
srt==3.5.3
xlrd==2.0.1
</pre>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Further Methodological Details on Selecting Occupations</h3>
<div class="ltx_para ltx_noindent" id="A1.SS7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Assigning Occupations to Sectors.
</span>We assigned occupations to sectors by using the 2023 BLS National Employment Matrix from <cite class="ltx_cite ltx_citemacro_cite">U.S. Bureau of Labor
Statistics (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib22" title="">2025a</a>)</cite> to identify the sector with the highest employment for each occupation. This involved filtering to “Line Item” occupations, taking the first two digits of NAICS codes, dropping “total employment” rows, summing 2023 employment, and assigning each occupation to the sector with the largest share of employment.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Detail about O*NET Data Source</span></p>
</div>
<section class="ltx_paragraph" id="A1.SS7.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Occupations in GDPval.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS0.Px1.p1">
<p class="ltx_p">We arrived at 831 occupations by filtering to “Detailed” occupations from the May 2024 OEWS national employment and wage statistics <cite class="ltx_cite ltx_citemacro_cite">U.S. Bureau of Labor
Statistics (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib23" title="">2025b</a>)</cite> to exclude any aggregate employment categories. We dropped “All Other” occupations, which are catch-all categories within a broader group that bundle together occupations that don’t fit into any of the detailed occupations in that group. Dropping “All Other” occupations left us with 761 occupations.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS7.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Calculation of Total Wages Earned by Occupation.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS0.Px2.p1">
<p class="ltx_p">Estimated total wages earned is calculated as total employment * mean annual salary for jobs with annual salaries, and total employment * hourly salary * typical work year of 2080 hours for jobs with only hourly salaries. The determination of which jobs had annual vs. hourly salaries was included in O*NET data. 2080 hours is cited as a “typical work year” by the Bureau of Labor Statistics (BLS), assuming someone works 40 hours per week. This is an imperfect estimate (eg., the BLS acknowledges actors “generally do not work 40 hours per week, year round”) but is the most precise estimate provided by the BLS.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS7.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Classifying Occupations as Digital.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS0.Px3.p1">
<p class="ltx_p">To classify occupations as predominantly digital, we use a task-based approach. For many occupations, the O*NET database contains task statements and ratings that list all the tasks performed by a worker in an occupation.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Note that while O*NET distinguishes between Core and Supplemental tasks in its task data, we treat these two task types equally in our calculation of task share.</span></span></span> The O*NET data is provided on the 6-digit SOC occupational code level (SOC-6). We map the O*NET SOC-6 occupations and the corresponding tasks to occupations in the OEWS dataset which reports wages at the 4-digit SOC level (“SOC-4”). For each SOC-4 occupation, we classify its tasks as either digital or non-digital using a prompted GPT-4o model that receives both the occupation and task. We then calculate the weighted share of digital tasks for each occupation. Occupations are classified as digital if their digital share exceeds a threshold of 0.60.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS0.Px3.p2">
<p class="ltx_p">To calculate the weights for our weighted task share, we use task ratings data from O*NET surveys, which includes the relevance, frequency, and importance of each task of the occupation.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>For the two occupations without O*NET 28.3 task ratings (“Facilities Managers” and “Medical Dosimetrists”), we used task ratings from O*NET 29.0.</span></span></span> We first calculate an Adjusted Task Score for each combination of 6-digit SOC occupation and task. This score is defined as the simple average of the three normalized task ratings: task frequency, task importance, and task relevance. Each rating is normalized relative to the maximum observed rating (e.g. the importance ratings are out of 5).<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>The maximum frequency value is 7, the maximum importance value is 5, and the maximum relevance value is 100.</span></span></span> If one of these ratings is missing for a task, we impute the value with the mean of that rating across all tasks within the same occupation. For example, if a task lacks a frequency rating, we assign it the average normalized frequency rating of all tasks in the occupation.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS0.Px3.p3">
<p class="ltx_p">We then aggregate these 6-digit Adjusted Task Scores into 4-digit Adjusted Task Scores (for each set of 4-digit SOC occupations and tasks). We do this by summing the SOC-6 Adjusted Task Scores of SOC-6 occupations within a SOC-4 occupation for each task.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>If a SOC-4 occupation is mapped to one SOC-6 occupation, the SOC-6 and SOC-4 Adjusted Task Scores are the same.</span></span></span> For example, the SOC-4 occupation Computer Occupations, All Other combines two 6-digit SOC occupations (<span class="ltx_text ltx_font_italic">Information Security Engineers</span> and <span class="ltx_text ltx_font_italic">Penetration Testers</span>) which have one task in common: “Identify security system weaknesses, using penetration tests.” This task has two SOC-6 Adjusted Task Scores which are added together to create the SOC-4 Adjusted Task Score.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS0.Px3.p4">
<p class="ltx_p">Next, we calculate the Weighted Task Share for each combination of 4-digit SOC occupation and task. The Weighted Task Share is the Adjusted Task Score of the occupation-task pair divided by the sum of Adjusted Task Scores of that occupation. For each occupation, the sum of Weighted Task Share across all its tasks is equal to one. The Weighted Task Share gives us a measure of the relative significance of each task for a given occupation. These Weighted Task Shares are the weights used to calculate the weighted share of digital tasks for each occupation.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS7.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Handling Missing Data.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS0.Px4.p1">
<ol class="ltx_enumerate" id="A1.I7">
<li class="ltx_item" id="A1.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I7.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Missing Task Statements. </span> Some occupations in OEWS lacked associated task statements or ratings. Forty-seven of these were broad “All Other” categories without component tasks <span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>These occupations were: Entertainers and Performers, Sports and Related Workers, All Other; Postsecondary Teachers, All Other; Production Workers, All Other; Office and Administrative Support Workers, All Other; Teachers and Instructors, All Other; Surgeons, All Other; Information and Record Clerks, All Other; Community and Social Service Specialists, All Other; Educational Instruction and Library Workers, All Other; Sales and Related Workers, All Other; Education Administrators, All Other; Social Workers, All Other; Legal Support Workers, All Other; Food Preparation and Serving Related Workers, All Other; Personal Care and Service Workers, All Other; Food Processing Workers, All Other; Motor Vehicle Operators, All Other; Financial Clerks, All Other; Media and Communication Workers, All Other; Counselors, All Other; Social Sciences Teachers, Postsecondary, All Other; First-Line Supervisors of Protective Service Workers, All Other; Dentists, All Other Specialists; Material Moving Workers, All Other; Helpers, Construction Trades, All Other; Drafters, All Other; Media and Communication Equipment Workers, All Other; Metal Workers and Plastic Workers, All Other; Cooks, All Other; Designers, All Other; Life Scientists, All Other; Building Cleaning Workers, All Other; Precision Instrument and Equipment Repairers, All Other; Grounds Maintenance Workers, All Other; Religious Workers, All Other; Artists and Related Workers, All Other; Textile, Apparel, and Furnishings Workers, All Other; Gambling Service Workers, All Other; Transportation Workers, All Other; Extraction Workers, All Other; Entertainment Attendants and Related Workers, All Other; Woodworkers, All Other; Underground Mining Machine Operators, All Other; Agricultural Workers, All Other; Logging Workers, All Other; Rail Transportation Workers, All Other; Communications Equipment Operators, All Other.</span></span></span>; twelve others were split into finer sub-occupations in O*NET 29.0 (as of August 2025). For the latter, we incorporated the full set of component tasks from their sub-occupations in O*NET 29.0. The exact reconciliation of how we mapped these 12 occupations is below:</p>
<ol class="ltx_enumerate" id="A1.I7.i1.I1">
<li class="ltx_item" id="A1.I7.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Tour and Travel Guides: </span> This SOC Code is broken out into two occupations: <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/39-7011.00." title="">Tour Guides and Escorts</a> and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/39-7012.00" title="">“Travel Guides”</a>. We added the tasks from both occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Miscellaneous Construction and Related Workers: </span>
This SOC Code is broken out into three occupations: <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/47-4091.00" title="">“Segmental Pavers”</a>, <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/47-4099.03" title="">“Weatherization Installers and Technicians”</a>, and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/47-4099.00" title="">“Construction and Related Workers, All Other”</a>. We added all of the tasks from “Segmental Pavers” and “Weatherization Installers.” “Construction and Related Workers, All Other” is a general occupation category without component tasks.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Teaching Assistants: </span>
This SOC Code is broken out into three occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/25-9043.00" title="">Teaching Assistants, Preschool, Elementary, Middle, and Secondary School, Except Special Education</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/25-9044.00" title="">Teaching Assistants, Special Education</a>,
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/25-9049.00" title="">Teaching Assistants, All Other</a>.
We added the tasks from Teaching Assistants, Preschool, Elementary, Middle, and Secondary School, Except Special Education, and Teaching Assistants, Special Education. Teaching Assistants, All Other is a general occupation category without component tasks.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Buyers and Purchasing Agents: </span>
This SOC Code is broken out into three occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/13-1021.00" title="">Buyers and Purchasing Agents, Farm Products</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/13-1022.00" title="">Wholesale and Retail Buyers, Except Farm Products</a>,
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/13-1023.00" title="">Purchasing Agents, Except Wholesale, Retail, and Farm Products</a>.
We added the tasks from the three occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(e)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Substance Abuse, Behavioral Disorder, and Mental Health Counselors: </span>
This SOC Code is broken out into two occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/21-1011.00" title="">Substance Abuse and Behavioral Disorder Counselors</a>
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/21-1014.00" title="">Mental Health Counselors</a>.
We added the tasks from both occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(f)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Clinical Laboratory Technologists and Technicians: </span>
This SOC Code is broken out into six occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/29-2011.00" title="">Medical and Clinical Laboratory Technologists</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/29-2011.01" title="">Cytogenetic Technologists</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/29-2011.02" title="">Cytotechnologists</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/29-2011.04" title="">Histotechnologists</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/29-2012.00" title="">Medical and Clinical Laboratory Technicians</a>,
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/29-2012.01" title="">Histology Technicians</a>.
We added the tasks from all these occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(g)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Special Education Teachers, Kindergarten and Elementary School: </span>
This SOC Code is broken out into two occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/25-2055.00" title="">Special Education Teachers, Kindergarten</a>
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/25-2056.00" title="">Special Education Teachers, Elementary School</a>.
We added the tasks from both occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(h)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Home Health and Personal Care Aides: </span>
This SOC Code is broken out into two occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/31-1121.00" title="">Home Health Aides</a>
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/31-1122.00" title="">Personal Care Aides</a>.
We added the tasks from both occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i9.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Property Appraisers and Assessors: </span>
This SOC Code is broken out into two occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/13-2023.00" title="">Appraisers and Assessors of Real Estate</a>
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/13-2022.00" title="">Appraisers of Personal and Business Property</a>.
We added the tasks from both occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(j)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i10.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Miscellaneous Assemblers and Fabricators: </span>
This SOC Code is broken out into two occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/51-2099.00" title="">Assemblers and Fabricators, All Other</a>
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/51-2092.00" title="">Team Assemblers</a>.
We match to Team Assemblers since Assemblers and Fabricators, All Other is a general occupation category without component tasks.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(k)</span>
<div class="ltx_para" id="A1.I7.i1.I1.i11.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Electrical, Electronic, and Electromechanical Assemblers, Except Coil Winders, Tapers, and Finishers: </span>
This SOC Code is broken out into two occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/51-2022.00" title="">Electrical and Electronic Equipment Assemblers</a>
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/51-2023.00" title="">Electromechanical Equipment Assemblers</a>.
We added the tasks from both occupations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I7.i1.I1.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(l)</span>
<div class="ltx_para ltx_noindent" id="A1.I7.i1.I1.i12.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">First-Line Supervisors of Transportation and Material Moving Workers, Except Aircraft Cargo Handling Supervisors: </span>
This SOC Code is broken out into four occupations:
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/53-1042.00" title="">First-Line Supervisors of Helpers, Laborers, and Material Movers, Hand</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/53-1043.00" title="">First-Line Supervisors of Material-Moving Machine and Vehicle Operators</a>,
<a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/53-1044.00" title="">First-Line Supervisors of Passenger Attendants</a>,
and <a class="ltx_ref ltx_href" href="https://www.onetonline.org/link/summary/53-1049.00" title="">First-Line Supervisors of Transportation Workers, All Other</a>.
We added the tasks from First-Line Supervisors of Helpers, Laborers, and Material Movers, Hand, First-Line Supervisors of Material-Moving Machine and Vehicle Operators, and First-Line Supervisors of Passenger Attendants. First-Line Supervisors of Transportation Workers, All Other is a general occupation category without component tasks.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A1.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="A1.I7.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Missing Task Ratings.</span> There are 36 SOC-6 occupations which do not have any task rating in O*NET 28.3 or 29.0. These correspond to 34 SOC-4 occupations.<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>These SOC-4 occupations are: Aircraft Service Attendants, Bus Drivers, School, Calibration Technologists and Technicians, Cardiologists, Crematory Operators, Data Scientists, Disc Jockeys, Except Radio, Emergency Medical Technicians, Emergency Medicine Physicians, Entertainment and Recreation Managers, Except Gambling, Financial and Investment Analysts, Financial Risk Specialists, First-Line Supervisors of Entertainment and Recreation Workers, Except Gambling Services, First-Line Supervisors of Security Workers, Fundraising Managers, Health Information Technologists and Medical Registrars, Hydrologic Technicians, Legislators, Lighting Technicians, Medical Records Specialists, Orthopedic Surgeons, Except Pediatric, Paramedics, Pediatric Surgeons, Project Management Specialists, Public Relations Managers, Sales Representatives of Services, Except Advertising, Insurance, Financial Services, and Travel, School Bus Monitors, Shuttle Drivers and Chauffeurs, Software Developers, Special Education Teachers, Kindergarten and Elementary School, Substitute Teachers, Short-Term, Taxi Drivers, Teaching Assistants, Except Postsecondary, Web and Digital Interface Designers.</span></span></span> Among these, 2 SOC-4 occupations (Data Scientists and Web and Digital Interface Designers) have task ratings for some of the component SOC-6 occupations which allow us to compute the Adjusted and Weighted Task Share measures. For the rest of the 32 SOC-4 occupations that have no O*NET task ratings, we cannot compute the Adjusted or Weighted Task Share measure. Instead, we proxy the Weighted Task Share as follows: for each combination of 4-digit SOC occupation and task, we calculate the number of times the task appears (i.e., task frequency) for the occupation and divide by the sum of task frequency of all tasks of that occupation. For example, the 4-digit SOC occupation “Special Education Teachers, Kindergarten and Elementary School” combines two 6-digit SOC occupations (<span class="ltx_text ltx_font_italic">Special Education Teachers, Elementary School</span> and <span class="ltx_text ltx_font_italic">Special Education Teachers, Kindergarten</span>) has 43 unique tasks. Among these 17 tasks appear twice. Thus, the sum of task frequency across 43 tasks is 60. For each task that appears once, the proxy Weighted Task Share is 1/60 = 0.0017, and for each task that appears twice, the proxy Weighted Task Share is 2/60 = 0.0033.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.7.1 </span>Validating the Digital Tasks Measure</h4>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS1.p1">
<p class="ltx_p">We benchmark our “knowledge work” classification method against the task-content framework of <cite class="ltx_cite ltx_citemacro_cite">Acemoglu &amp; Autor (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib2" title="">2011</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS1.p2">
<p class="ltx_p">The framework in <cite class="ltx_cite ltx_citemacro_cite">Acemoglu &amp; Autor (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib2" title="">2011</a>)</cite> is based on the U.S. Department of Labor’s O*NET survey, which collects data on the activities, work “content”, and abilities required for each occupation. The framework aggregates these measures into five scores:</p>
<ol class="ltx_enumerate" id="A1.I8">
<li class="ltx_item" id="A1.I8.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I8.i1.p1">
<p class="ltx_p">Non-routine cognitive: Analytical.</p>
</div>
</li>
<li class="ltx_item" id="A1.I8.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I8.i2.p1">
<p class="ltx_p">Non-routine cognitive: Interpersonal.</p>
</div>
</li>
<li class="ltx_item" id="A1.I8.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I8.i3.p1">
<p class="ltx_p">Routine cognitive.</p>
</div>
</li>
<li class="ltx_item" id="A1.I8.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I8.i4.p1">
<p class="ltx_p">Routine manual.</p>
</div>
</li>
<li class="ltx_item" id="A1.I8.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="A1.I8.i5.p1">
<p class="ltx_p">Non-routine manual physical.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS1.p3">
<p class="ltx_p">Each score is computed as a composite measure of select O*NET “Importance” scales. For example, the “Non-routine cognitive: Analytical” score for each occupation is computed by summing the (normalized) values of the “Analyzing data/information” work activity, the “Thinking creatively” work activity, and the “Interpreting information for others” work activity. A high numerical value for an occupation for a given score indicates that the occupation relies heavily on that type of work.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS1.p4">
<p class="ltx_p">We compute the <cite class="ltx_cite ltx_citemacro_cite">Acemoglu &amp; Autor (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib2" title="">2011</a>)</cite> scores for each occupation and then compare them with our measures of knowledge work (that is, the share of digital tasks and a binary “knowledge work” indicator for each occupation).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS1.p5">
<p class="ltx_p">In our first set of results, we compare each <cite class="ltx_cite ltx_citemacro_cite">Acemoglu &amp; Autor (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib2" title="">2011</a>)</cite> task-content score with the share of digital tasks in an occupation. The patterns are clear: occupations with higher digital-task shares score systematically higher on the non-routine cognitive dimensions and lower on the manual dimensions. In other words, the more an occupation relies on digital tasks, the more it resembles cognitive, non-routine work.</p>
</div>
<figure class="ltx_figure" id="A1.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="93" id="A1.F17.g1" src="assets/acemoglu_task_scatterplot.png" width="494"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Distribution of occupations and task contents</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS7.SSS1.p6">
<p class="ltx_p">In our second set of results, we look at the relationship between the <cite class="ltx_cite ltx_citemacro_cite">Acemoglu &amp; Autor (<a class="ltx_ref" href="https://arxiv.org/html/2510.04374v1#bib.bib2" title="">2011</a>)</cite> scores and our binary measure of “knowledge work.” In the following figure, we plot each occupation’s value for each score, and color occupations by the paper’s knowledge-work classification: blue for occupations identified as knowledge work and red for all others. The pattern is again clear–knowledge-work occupations cluster at the top of the non-routine cognitive distributions and at the bottom of the routine and manual distributions. Taken together, these results suggest that our digital-task classification is closely aligned with the economic literature on cognitive/manual work.</p>
</div>
<figure class="ltx_figure" id="A1.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="169" id="A1.F18.g1" src="assets/acemoglu_occupationcontent.png" width="274"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Scatterplot of digital tasks and task contents</figcaption>
</figure>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Oct  5 21:31:03 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
